<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h3>Bug</h3>

<p>This was probably a posting or a copy/paste bug - in <code>create_file()</code> you meant to name the first parameter to be <code>filename</code> instead of <code>title</code>.</p>

<h3>Code Style and PEP8 notes</h3>

<ul>
<li>use <em>more descriptive variable names</em> - <code>r</code>, <code>s</code>, <code>c</code>, <code>v</code> are not good variable names</li>
<li>remove unused <code>TimeoutException</code> import, organize imports in groups and alphabetically (<a href="https://www.python.org/dev/peps/pep-0008/#imports" rel="nofollow noreferrer">PEP8 reference</a>)</li>
<li>move the comments before the functions into proper <em>docstrings</em></li>
<li><a href="https://stackoverflow.com/q/14797375/771848">avoid catching broad exceptions via bare <code>except</code></a></li>
<li>follow <a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow noreferrer">PEP8 whitespace and newline usage guidelines</a> - specifically, whitespaces around the operators, two newlines between the function definitions </li>
</ul>

<h3>Third-party Library Usage notes</h3>

<ul>
<li><code>BeautifulSoup</code> constructor accepts a file-like object as well - you don't have to explicitly call <code>.read()</code> method</li>
<li><p>it is a good idea to specify <code>BeautifulSoup</code>'s parser explicitly:</p>

<pre><code>soup = BeautifulSoup(r, "html.parser")  # or "html5lib", or "lxml"
</code></pre>

<p>Otherwise, <code>BeautifulSoup</code> would pick a parser automatically depending on what is available. This would mean that on your machine it may pick, say, <code>lxml</code> - on someone else's - <code>html5lib</code>, on other's <code>html.parser</code>. And, there might be different results since <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers" rel="nofollow noreferrer">different parser behave differently</a>.</p></li>
<li><p>a bigger problem is that you are not actually using <code>BeautifulSoup</code> capabilities at all - you are post-processing the "soup" with regular expressions. It looks like you can achieve the same by calling the <code>.get_text()</code> method on the "soup" object</p></li>
<li><p>for better reusability I would probably assume <code>chromedriver</code> is in <code>PATH</code> instead of hardcoding the path to it explicitly</p></li>
<li><p>you can reuse <code>WebDriverWait</code> instance instead of re-instantiating it multiple times</p></li>
<li><p>you <em>never close the driver instance</em></p></li>
<li><p><code>time.sleep()</code> is generally unreliable when used for waiting for something with <code>selenium</code> - recheck if you actually need it </p></li>
</ul>

<h3>Code Organization notes</h3>

<p>You are passing <code>driver</code> to multiple methods - it may be a good idea to have a class with a <code>driver</code> and <code>wait</code> attributes for better reusability.</p>

<h3>Some Syntax Sugar</h3>

<ul>
<li><p>you can construct a list of videos with a list comprehension:</p>

<pre><code>[(video.text, video.get_attribute("href"))
 for video in driver.find_elements_by_class_name("yt-uix-tile-link")]
</code></pre></li>
<li><p>you can unpack the list of videos into filename and a link:</p>

<pre><code>for filename, link in videos:
</code></pre></li>
</ul>

<h3>Modified Code:</h3>

<pre><code>import sys
import time
import urllib.request

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait


class YoutubeSubtitlesScraper:
    def __enter__(self):
        self.driver = webdriver.Chrome()

        self.wait = WebDriverWait(self.driver, 10)

        self.driver.get(start_url)
        self.display_all_videos()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.driver.close()

    def display_all_videos(self):
        """Clicks on "Load More" button to display all users videos."""
        while True:
            try:
                element = self.wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "yt-uix-load-more")))
                element.click()
            except TimeoutException:
                break

    def subtitles(self):
        """Visits video's page, enables 'CC' to scrape the subtitles and generates filename, link and the subtitles content."""
        videos = [(video.text, video.get_attribute("href"))
                  for video in self.driver.find_elements_by_class_name("yt-uix-tile-link")]

        for filename, link in videos:
            self.driver.get(link)
            self.enable_subtitles()

            link = self.get_subtitles_link()
            yield filename, link, self.scrape_subtitles(link) if link else "No Closed Caption"

    def enable_subtitles(self):
        """Clicks on CC(Closed Caption) button in YouTube video."""
        show_subtitles_button = self.wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "ytp-subtitles-button")))
        show_subtitles_button.click()

    def get_subtitles_link(self):
        """Finds string in performance timings that contains the substring 'srv3' which is the subtitles link."""
        time.sleep(1)
        timings = self.driver.execute_script("return window.performance.getEntries();")

        for timing in timings:
            for value in timing.values():
                if "srv3" in str(value):
                    return value
        return ""

    def scrape_subtitles(self, subtitle_link):
        """HTML parses subtitles."""
        response = urllib.request.urlopen(subtitle_link)
        soup = BeautifulSoup(response, "lxml")
        return soup.get_text(strip=True)


def create_file(filename, link, subtitles):
    """Creates file for the subtitle."""
    title = "".join([c for c in filename if c.isalpha() or c.isdigit() or c == ' ']).rstrip()

    with open(title + '.txt', 'w') as subtitles_file:
        subtitles_file.write('LINK: ' + link + '\n')
        subtitles_file.write(subtitles)


if __name__ == "__main__":
    start_url = sys.argv[1]

    with YoutubeSubtitlesScraper(start_url) as scraper:
        for filename, link, content in scraper.subtitles():
            try:
                create_file(filename, link, content)
            except:
                print("Can't create file for: " + filename + " : " + link)
</code></pre>

<p>(not tested)</p>

<p>Note how we made the class a <em>context manager</em> by defining the <code>__enter__()</code> and <code>__exit__()</code> magic methods.</p>
    </div>