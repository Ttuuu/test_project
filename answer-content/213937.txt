<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Running <code>ssh</code> in a loop is not efficient. Since the script is not interactive,
you could pass the entire script to a remote Bash process on <code>stdin</code>,
so that loop will run entirely on the remote server, locally:</p>

<pre><code>ssh remotehost bash &lt;&lt; "EOF"
mydirs=(/var/www/files /var/www/photos /var/www/info)
for d in ${mydirs[@]}; do
   test -d $d
   res=$?
   test $res -ne 0 &amp;&amp; { mkdir -p $d; res=$?; }
   test $res -ne 0 &amp;&amp; { echo "error during mkdir on remote"; exit 1; }
done
EOF
</code></pre>

<p>Notice the double-quotes around the here document label, this is to avoid variable interpolation. The entire script is passed to the remote shell literally.</p>

<p>I made only the minimal changes to illustrate the point. Some important improvements are well advised:</p>

<ul>
<li>All variables used as command line arguments should be double-quoted: <code>"${mydirs[@]}"</code>, <code>"$d"</code>, and so on.</li>
<li>As a comment mentioned, when using <code>mkdir -p</code>, it's unnecessary to test if the directory exists.</li>
<li>Now that the main operations run in a single process, the pipeline can be simplified.</li>
<li>As another comment mentioned, consider investing in learning a proper system administration tool such as Puppet, Ansible, Chef, or similar.</li>
</ul>
    </div>