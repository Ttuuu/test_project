<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h2>The not so arbitrary Network</h2>

<p>Your original claim was that your network is "arbitrary". From what I see, I would tend to say that it's not so arbitrary as one might expect.</p>

<p>Arbitrary:</p>

<ul>
<li>number of neurons per layer (you call it "structure")</li>
<li>activation function</li>
<li>their derivatives (which means they are not totally arbitrary)</li>
</ul>

<p>Not arbitrary:</p>

<ul>
<li>learning rate (<code>0.1</code>)</li>
<li>network weight initialization</li>
<li>error function (Somehow set to half of the squared Euclidean distance? Maybe you wanted the <a href="https://github.com/keras-team/keras/blob/master/keras/losses.py#L13" rel="nofollow noreferrer">Mean Squared Error</a> here?)</li>
<li>number of epochs in training (<code>10000</code>)</li>
<li>training mode (<a href="https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean" rel="nofollow noreferrer">sample vs. batch learning</a>)</li>
<li>training termination condition (<code>error_delta &lt; 1.0e-6</code>)</li>
<li>optimizer</li>
<li>...</li>
</ul>

<p>Those are all (more or less) important aspects one might like to tune without "<a href="https://stackoverflow.com/questions/5626193/what-is-monkey-patching">monkey-patching</a>" them into the original class (if possible). Some of these parameters could easily be set in the constructor or when calling <code>train</code> (for number of epochs etc.). The error function could also easily be set at initialization, especially since it's already a <code>@staticmethod</code> which does not rely on class internals.</p>

<p>I assume a flexible optimizer would be out of scope for what you want to do (and there are a <a href="https://keras.io/" rel="nofollow noreferrer">plethora</a> <a href="https://caffe.berkeleyvision.org/" rel="nofollow noreferrer">of</a> <a href="https://pytorch.org/" rel="nofollow noreferrer">frameworks</a> that can do this).</p>

<h2>The code itself</h2>

<p>The initialization of the network weights and biases is using a <code>while</code> loop and is unnecessarily complicated. You don't need a <code>while</code> loop here since you know exactly how many iterations have to be done. So use a <code>for</code> loop instead, which would lead you to:</p>

<pre class="lang-py prettyprint-override"><code>for i in range(len(structure)-1):
    self.w.append(np.random.uniform(low=-1.0, high=1.0, size=(structure[i], structure[i+1])))
    self.b.append(np.random.uniform(low=-1.0, high=1.0, size=(structure[i+1])))
</code></pre>

<p>Apart from not needing to keep tracking of the indexing variable, you also don't need indexing tricks with negatives indices and you are able to use <code>.append(...)</code> instead of inserting at the front.</p>

<p>Oh and while we are at initialization: there is no need to convert the functions/derivative tuples to lists. You can iterate over/index tuples the same way as over lists.</p>

<hr>

<p>Speaking of iterations, this</p>

<pre class="lang-py prettyprint-override"><code>layer_delta = []
iter_zi = iter(zip(reversed(range(len(z))), reversed(z)))
layer_delta.insert(0, (a[-1] - yi) * self.df[1](next(iter_zi)[1]))
for i, zi in iter_zi:
    delta_i = np.multiply(layer_delta[0] @ self.w[i+1].T, self.df[i](z[i]))
    layer_delta.insert(0, delta_i)
</code></pre>

<p>is madnessâ„¢ IMHO (and there is also likely a bug with <code>self.df[1]</code>). See <a href="https://stackoverflow.com/questions/529424/traverse-a-list-in-reverse-order-in-python">this SO post</a> on how to reverse enumerate a Python list, if you really think you need to. I don't think you need to because you use the actual list value only once and the index in all other cases. That would bring that monster down to something like</p>

<pre class="lang-py prettyprint-override"><code>layer_delta = [(a[-1] - yi) * self.df[-1](z[-1])]
for i in reversed(range(len(z)-1)):
    delta_i = np.multiply(layer_delta[0] @ self.w[i+1].T, self.df[i](z[i]))
    layer_delta.insert(0, delta_i)
</code></pre>

<p>You can and should adapt your other appearances of the monster above as well. Work carefully while refactoring these parts of your code, I cannot guarantee that I didn't make a mistake while deciphering them.</p>

<hr>

<p>The implementation of the forward pass is also a little bit to complicated IMHO. Since you're only going through the network once, there is no need to store the activation and output of all layers. You only need the last one.</p>

<p>You also implement the forward pass twice, in it's own <code>forward</code> function and in <code>train</code>. If you stick to your original implementation, think about if you would like to return the other values as well. That would leave you with only one piece of code to fix if something is wrong. If you're concerned about usability since the application phase now sees all the internal values, you could implement an internal function , e.g. <code>_forward</code> which does the heavy-lifting and let <code>forward</code> just return the final output.</p>

<hr>

<p>The comment and variable names at</p>

<pre class="lang-py prettyprint-override"><code># Convergence testing, according to the last N errors:
error_delta = sum(reversed(errors[-5:]))
if error_delta &lt; 1.0e-6:
    print("Error delta reached, ", epoch, " exiting.")
    break
</code></pre>

<p>are also a little off. The termination criterion does seem to check if the sum of the last N (where N is hard coded to 5) is below your (arbitrarily chosen) threshold. Also, there is no need to reverse the list here since <a href="https://en.wikipedia.org/wiki/Commutative_property" rel="nofollow noreferrer">sum does not care about the order</a>.</p>

<p>Since you are working in Python 3, you can also use the new f-string syntax for output formatting:</p>

<pre class="lang-py prettyprint-override"><code>print(f"Error delta reached in {epoch} exiting.")
</code></pre>

<hr>

<p>Speaking of comments, the methods of your class all lack user visible documentation. For that, Python programmers usually use so called <a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow noreferrer"><code>"""doc strings"""</code></a> on their methods/classes/functions. As an example:</p>

<pre class="lang-py prettyprint-override"><code>def train(self, training_data, labels):
    """Train the neural network with all the available data

    The training continues until the maximum number of epochs is reached or
    the termination criterion is hit.
    """
    # your code here ...
</code></pre>

<p>Documentation written in this kind of format will be picked up by all major Python IDEs as well as by Python's built-in <code>help(...)</code> function.</p>

<hr>

<p>I know there are common naming conventions in the neural network community, and when you implement it, you should stick to them as closely as possible which you mostly do. <strong>But</strong> bear in mind not to sacrifice the clarity and readability of your code. E.g. <code>z</code> could become <code>activation</code> and <code>a</code> could also be named <code>layer_output</code> (plurals might apply where they are used as list).</p>
    </div>