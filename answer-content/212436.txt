<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Others are better equipped than I to talk about the quality of the Python, but I can tell you that seeks are expensive, that all random reads cost about the same as long as you're reading less than a block, and that read+stat is worse than reading two sequential blocks, because the stat reads a directory and not the file.  </p>

<p>I see about a 10% speedup if you hash the first 4k and use that as your key; another 5% from not computing total size (the only thing stat is needed for, once you drop size from the key):</p>

<pre><code>def find_dups (top_dir, **hashkwargs):
    t0 = time()
    top_dir = os.path.abspath(os.path.expanduser(top_dir))
    dups = {}
    numfiles = numskipped = totsize = 0
    for dirpath,_,fnames in os.walk(top_dir):
        for fname in fnames:
            fpath = os.path.join(dirpath,fname)
            try:
                with open(fpath,'rb') as f:
                    fhash = hash( f.read(4096) )
                while True:
                    if fhash not in dups:
                        # a new, unique file has been found.
                        dups[fhash] = [fpath]
                        break
                    # file is a duplicate, or hash collision occured.
                    if filecmp.cmp(fpath,dups[fhash][0],shallow=False):
                        # duplicate.
                        dups[fhash].append(fpath)
                        break
                    # hash collision on actually different files; rehash.
                    fhash += 1
            except OSError:
                numskipped += 1
                continue
            numfiles += 1
    return dups, numfiles, numskipped, totsize, top_dir, time()-t0
</code></pre>
    </div>