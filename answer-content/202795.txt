<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>There are many problems with your approach.</p>

<ul>
<li><p>It can be extremely unfair depending on your tree structure. do a depth first complete walk and list probabilities. root is probability <code>1</code>, in each level divide by number of dirs (including <code>.</code>), finally divide by number of (matching) files. list all these hit probabilities and compare them. if your tree is not balanced you will probably conclude to drop your approach</p></li>
<li><p>It will fail to find rare file patterns. The primary chance for a file to be found is depending on the directory structure only, only the last denominator is depending on the file pattern. so this approach will not scale, <code>limit</code> has to grow with the actual tree size and there is a good chance that you will fail even so. In a balanced tree you could fix that by going deep randomly once, then switching to a regular DFS walk. However this will not succeed when your patterns are not evenly distributed.</p></li>
</ul>

<p>I propose to do a database (simple file list or better) where you can efficiently match against your pattern and do the random select an all matches. You can update the database in the background. You will not be limited to glob patterns but can also do regex and also match path names.</p>
    </div>