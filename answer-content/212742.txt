<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I'll also begin with the low hanging fruit mentioned by Mateusz Konieczny. <a href="https://www.python.org/dev/peps/pep-0008/" rel="nofollow noreferrer">PEP8</a>/pylint/etc. your code. It is decently formatted, but there are some issues. Before even considering optimizing performance, you should first optimize for the person reading your code. Until you've profiled and determined that you need to add complexity (because speed is an issue), programmer productivity (specifically, the ability to quickly glance at your code and understand it) is paramount.</p>

<p>Also, you can often eek out a decent bit of performance by switching to Python 3. Perhaps your hardware prevents this, but it's usually a free performance win. If the math truly is this intensive, running under <code>pypy</code> might also give you a free performance boost.</p>

<p>But, have you profile this code? Do benchmarks indicate that this needs to be optimized? As it exists right now, I find it unlikely that even sequentially this is unable to process 50*8 inputs every 30 seconds (that's 13 ops/sec or 75ms per op, which seems reasonable). If <code>print("Will add more logic here")</code> is computationally intensive, why not just run it in a separate process instead of complicating this relatively simple API request parsing and math?</p>

<p>Running the API requests in parallel could be as simple as using a <a href="https://docs.python.org/3.7/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer"><code>multiprocessing.Pool</code></a> (you'll want to use it instead of threads because of the <a href="https://realpython.com/python-gil/" rel="nofollow noreferrer">GIL</a>):</p>

<pre><code>with Pool() as pool:
    for msg in api_messages:
        pool.apply(pwr_report_msg_decode, msg)
</code></pre>

<p>Although, unfortunately it's not quite that simple. You'd need to make <code>obj_list</code> a shared object (between the processes), which between processes has overhead for writing/read (because you need locks). Also, your API requests may already come in from a threaded context. If you were on python 3, <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer"><code>asyncio</code></a> could probably make expressing this logic a lot easier.</p>

<p>To remedy the locking issue, you may try creating a separate <code>multiprocessing.Process</code> for each of the 50 things. Then you dispatch the API message to the appropriate process via a queue:</p>

<pre><code>queues = [Queue() for _ in range(50)]
processes = [Process(target=handle_thing_readings, args=(queue,))
             for queue in queues]

for msg in api_messages:
    queues[msg['code']].put(msg)

def handle_thing_readings(queue):
    device = DevPwrInfo()
    while True:
        msg = queue.get()
        device.add_dev_pwr(msg['id'], msg['pwr'],
                           msg['valid_flag'])
</code></pre>

<p>This does require serializing <code>msg</code>, so you may want to replace the dictionary with a custom object that has <code>__slots__</code> defined. That said, there is still overhead, but this approach is likely better than locking.</p>

<p>All of the run around here should make it clear such patterns aren't too well suited for Python, especially if performance really is a concern. In my opinion, something like Go is much better suited for a task like this. Thanks to channels and goroutines, you could express all of this complex parallel logic in maybe 10ish lines of Go (and it has some pretty nifty runtime tools for analyzing performance, checking for deadlocks, etc.).</p>
    </div>