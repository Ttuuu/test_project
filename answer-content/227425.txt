<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>To briefly summarize your problem, for each of your column numbers (1-19) you have two vectors (<code>FV</code> and <code>CV</code>) of the same length. For each pair of elements, you can only pick one. You want to pick the elements such that the sum is as close as possible to the target value for that column stored in <code>LA</code>.</p>

<p>First, a bit of theory. If <code>FV</code> were filled with all 0's, then you can think of your problem as being equivalent to selecting the subset of elements in <code>CV</code> that sum as closely as possible to the value stored in <code>LA</code>. This is the famous Subset Sum Problem, which is NP-hard. Since the Subset Sum Problem is a special case of your problem, then we know your problem is also NP-hard. This means there is no known polynomial runtime algorithm that will always correctly solve your problem.</p>

<p>That being said, people routinely solve NP-hard problems in times that they find acceptable for their problem instances using integer programming software. You could define one binary decision variable <code>x[i]</code> for each row <code>i</code> of your dataset that takes value 1 if you use the FV value and value 0 if you take the CV value, as well as decision variable <code>o</code> for your objective value (the absolute value of the difference between your selected sum and the target value). Then you could define the following integer program:</p>

<pre><code>min  o
s.t. o &gt;= \sum_i { CV[i] + (FV[i]-CV[i])*x[i] } - LA[i]
     o &gt;= -\sum_i { CV[i] - (FV[i]-CV[i])*x[i] } + LA[i]
     x[i] binary
</code></pre>

<p>You can solve this in R using a package like the <code>lpSolveAPI</code> package:</p>

<pre><code>library(lpSolveAPI)
system.time(results &lt;- lapply(seq_len(ncol(FV)), function(k) {
  mod &lt;- make.lp(2, nrow(FV)+1)
  set.objfn(mod, c(1, rep(0, nrow(FV))))
  set.row(mod, 1, c(1, df_CV[,k]-df_FV[,k]))
  set.row(mod, 2, c(1, df_FV[,k]-df_CV[,k]))
  set.constr.type(mod, c(2, 2))  # &gt;= constraints
  set.type(mod, 1, "real")
  set.type(mod, 2:(nrow(FV)+1), "binary")
  set.rhs(mod, c(1, -1) * (sum(df_CV[,k]) - df_LA[,k]))
  lp.control(mod, timeout=100)
  solve(mod)
  list(selections = 1+get.variables(mod), gap = get.objective(mod))
}))
#    user  system elapsed 
#  26.291   0.040  26.396 
</code></pre>

<p>So there's some good news -- for the problem instance you shared here, we can actually get the optimal selections very quickly (about 1 second per column). This is clearly much faster than a brute force approach, which would need to check 2^205 (more than a trillion trillion trillion trillion trillion) possibilities -- clearly not feasible. It is both faster than the approach you posted (since you were checking many randomly generated solutions), and also has the advantage of always guaranteeing that it gives you the best combination of CV and FV values possible.</p>

<p>The optimal solutions are very good; all give (up to numerical precision) sums equal to the target:</p>

<pre><code>sapply(results, "[[", "gap")
# [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
</code></pre>

<p>I returned the results as a list of lists, which I think should be more manageable than what you were originally returning (a matrix with a pair of columns for each original column, and each column in the pair meaning some different thing). You can build a matrix of selections for each row and column with:</p>

<pre><code>sapply(results, "[[", "selections")
</code></pre>

<p>If you increase the number of samples (the length of <code>CV</code> and <code>LV</code>), then you may find that the integer program may begin to take too long to solve to optimality. I included a timeout of 100 seconds per model in my code above; you can play with that to meet your needs.</p>
    </div>