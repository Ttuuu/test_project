<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>There is a nice trick to get the eigenvalues and eigenvectors of the covariance matrix <em>without</em> ever forming the matrix itself. This can be done using the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="nofollow noreferrer">singular value  decomposition (SVD)</a>, as described in <a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca/134283#134283">this post from Stats.SE</a>. Not only is this more numerically stable, but the results are automatically sorted.</p>

<p>A python version might look like this:</p>

<pre><code>def components(X):
    _, vals, vecs = np.linalg.svd(X - X.mean(axis=0), full_matrices=False)
    return vals**2/(len(X)-1), vecs
</code></pre>

<p>A few things to note:</p>

<ul>
<li>As described in the linked post above, the <a href="https://en.wikipedia.org/wiki/Design_matrix" rel="nofollow noreferrer">data matrix</a> is typically defined with dimensions as columns, i.e. the transpose of your <code>X</code>.</li>
<li>The principle values and components are typically sorted from largest to smallest, i.e. the reverse of yours.</li>
<li>The function above does not assume that <code>X</code> has been pre-centered.</li>
</ul>

<p>So to get results comparable to yours, you would need to do:</p>

<pre><code>vals, vecs = components(X.T)
e_vals, e_vecs = vals[::-1], vecs[::-1]
</code></pre>
    </div>