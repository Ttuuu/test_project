<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Whereas unlikely to have high impact, I have found a potential source of overfitting in your code:</p>
<pre><code># lets normalize the data
def normalize(input_data):
    minimum = input_data.min(axis=0)
    maximum = input_data.max(axis=0)
    #normalized = (input_data - minimum) / ( maximum - minimum )
    normalized = preprocessing.normalize(input_data, norm='l2')
    return normalized
</code></pre>
<p>When training a model, you should always consider the complete pipeline. Everywhere, where dataset properties are used to adapt the pipeline, only training data should be used.</p>
<p>The preprocessing step - the normalization - needs to be trained as well. Therefore you would have to fit it with training data and the transform on test (without using the min and max of test data).</p>
<p>Data Leakage as in using test data properties in your model can result in overfitting.</p>
<p>See <a href="https://medium.com/@ODSC/how-to-fix-data-leakage-your-models-greatest-enemy-e34fa26abac5" rel="nofollow noreferrer">Medium</a> and <a href="https://datascience.stackexchange.com/questions/29055/data-snooping-information-leakage-when-performing-feature-normalization">datascience.stackexchange</a> for details such as:</p>
<blockquote>
<p>Most practitioners — including myself — typically drop their full
dataset into the same collection and normalize it all at once before
splitting the data into test and evaluation. While the code for this
approach will be cleaner, this breaks fundamental assumptions about
data leakage. Most importantly, we are using information from data
that will appear in both the test and training data. This is because
our mean and standard deviation will be based on the full dataset, not
just the training data.</p>
<p>Some practitioners will normalize the two
datasets separately, using different means and standard deviations.
This is also incorrect since it breaks the assumption that the data is
drawn from the same distribution.</p>
<p>Mr. Guts tells us that in order to
remedy this, we must first separate our data into training and test
sets. Then, once we normalize the training set, we apply the mean and
standard deviation to the normalization of the test set. This is a
very subtle source of data leakage that most are apt to miss, but
important to creating the best machine learning model possible.</p>
</blockquote>
    </div>