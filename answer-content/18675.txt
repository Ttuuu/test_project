<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Take a look at the <a href="http://eventlet.net" rel="nofollow">eventlet</a> library. It'll let you write code that fetches all the web pages in parallel without ever explicitly implementing threads or locking. </p>

<pre><code>import cStringIO, threading, MySQLdb.cursors, pycurl

NUM_THREADS = 100
lock_list = threading.Lock()
lock_query = threading.Lock()
</code></pre>

<p>The purist that I am, I wouldn't make this locks globals.    </p>

<pre><code>db = MySQLdb.connect(host = "...", user = "...", passwd = "...", db = "...", cursorclass=MySQLdb.cursors.DictCursor)
cur = db.cursor()
cur.execute("SELECT...")
rows = cur.fetchall()
rows = [x for x in rows]  # convert to a list so it's editable
</code></pre>

<p>It would make more sense to do this sort of thing after you've define your classes. At least that would be python's convention.</p>

<pre><code>class MyThread(threading.Thread):
    def run(self):
        """ initialize a StringIO object and a pycurl object """
</code></pre>

<p>that's pretty much the most terrible description of this function I could have come up with. (You seem to be thinking of that as a comment, but by convention this should be a docstring, and describe the function)</p>

<pre><code>        while True:
            lock_list.acquire()  # acquire the lock to extract a url
            if not rows:  # list is empty, no more url to process
                lock_list.release()
                break
            row = rows.pop()
            lock_list.release()
</code></pre>

<p>It'd be a lot simpler to use a queue. It'd basically do all of that part for you.</p>

<pre><code>            """ download the page with pycurl and do some check """

            """ WARNING: possible bottleneck if all the pycurl
                connections are waiting for the timeout """

            lock_query.acquire()
            cur.execute("INSERT INTO ...")  # insert the full page into the database
            db.commit()
            lock_query.release()
</code></pre>

<p>It'd be better to put this data in another queue and have a database thread take care of it. This works, but I think the multi-queue approach would be cleaner.</p>

<pre><code>            """do some parse with BeautifulSoup using the StringIO object"""

            if something is not None:
                lock_query.acquire()
                cur.execute("INSERT INTO ...")  # insert the result of parsing into the database
                db.commit()
                lock_query.release()
</code></pre>

<p>Same here. Note that there is no point in python of trying to split up processing using threads. The GIL means you'll get no advatange.</p>

<pre><code># create and start all the threads
threads = []
for i in range(NUM_THREADS):
    t = MyThread()
    t.start()
    threads.append(t)

# wait for threads to finish
for t in threads:
    t.join()
</code></pre>
    </div>