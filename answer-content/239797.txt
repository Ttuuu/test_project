<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Two things about the way you handle files:</p>

<ol>
<li><p>Since you <code>open</code> the file and only <code>close</code> it at the end, if an exception disrupts your program in between, the file will not be properly closed. Instead use a <code>with</code> context manager:</p>

<pre><code>with open("bundesliga_table.csv", "w") as csv_file:
    ...
</code></pre>

<p>This automatically closes the file for you when leaving the block, whether by having finished the code within or due to an exception.</p></li>
<li><p>Currently you are writing one row at a time. However, the writer can also take an iterable of rows and write them all at once. This allows it to potentially optimize the writing in a better way.</p></li>
</ol>

<p>The latter also gives you the opportunity to put your parsing code into a function. Actually, you should probably put all your code into functions! This way you can give them nice and clear names, add a <a href="https://www.python.org/dev/peps/pep-0257/" rel="nofollow noreferrer"><code>docstring</code></a> describing what the function does and how to use it and add type annotations if you want to. This makes your code much more readable, especially when it grows to more than the few lines you currently have. It also decouples getting the data from displaying it and from writing it to a file, all of which are separate things that are mushed together in your current code. Again, that is fine for small scripts or as a starting point, but when your scripts get larger you want to refactor this.</p>

<p>You should also be careful with <code>try...except</code> clauses. The bare <code>except</code> you currently have will also catch e.g. the user pressing <kbd>Ctrl</kbd>+<kbd>C</kbd> if they want to abort the process (because it is taking too long or whatever). Using <code>except Exception</code> will avoid that at least, but you should catch as specific as possible.</p>

<p>In this case I would use something like this:</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import csv
from itertools import chain

def get_table(url):
    """Parse the official bundesliga website to get the current table.

    Returns an iterable of rows."""
    r = requests.get(url)
    r.raise_for_status()
    soup = BeautifulSoup(r.content, "lxml")
    for club in soup.find_all("tr"):
        try:
            rank = club.find("td", class_="rank").text
            team = club.find("span", class_="d-none d-lg-inline").text
            matches = club.find("td", class_="matches").text
            points = club.find("td", class_="pts").text
            difference = club.find("td", class_="difference").text
            yield rank, team, matches, points, difference
        except Exception:
            print("Did not find a team:")
            print(club)

def write_csv(rows, file_name):
    """Write an iterable of rows to the CSV file file_name."""
    with open(file_name, "w") as csv_file:
        writer = csv.writer(csv_file)
        writer.writerows(rows)

if __name__ == "__main__":
    url = "https://www.bundesliga.com/de/bundesliga/tabelle"
    rows = list(get_table(url))
    for row in rows:
        print(" ".join(map(str, row)))
    header = ["Rank", "Team", "Matches", "Points", "Goal Difference"]
    write_csv(chain(header, rows), "bundesliga_table.csv")
</code></pre>

<p>Note that I used a <a href="http://stackoverflow.com/questions/419163/what-does-if-name-main-do"><code>if __name__ == "__main__":</code> guard</a> to allow importing from this script from another script without the scraping being run. The <code>get_table</code> function returns a <a href="https://wiki.python.org/moin/Generators" rel="nofollow noreferrer">generator</a>, which you can just iterate over and it produces the values as it goes. But since we need the content both for printing and for writing, we need to persist it using <code>list</code>. It also has a <code>r.raise_for_status</code>, which will raise an exception if getting the webpage failed for whatever reason, which means you know right away that you are not connected to the internet and not only when it cannot parse the (not-existing) website.</p>
    </div>