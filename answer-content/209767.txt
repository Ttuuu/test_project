<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>By putting the CSV exporting logic into the spider itself, you are re-inventing the wheel and not using all the advantages of Scrapy and its components and, also, making the crawling slower as you are writing to disk in the crawling stage every time the callback is triggered. </p>

<p>As you mentioned, the <em>CSV exporter is built-in</em>, you just need to yield/return items from the <code>parse()</code> callback:</p>

<pre><code>import scrapy


class TorrentSpider(scrapy.Spider):
    name = "torrentdata"
    start_urls = ["https://yts.am/browse-movies?page={}".format(page) for page in range(2,20)] #get something within list

    def parse(self, response):
        for record in response.css('.browse-movie-bottom'):
            yield {
                "Name": record.css('.browse-movie-title::text').extract_first(default=''),
                "Year": record.css('.browse-movie-year::text').extract_first(default='')
            }
</code></pre>

<p>Then, by running:</p>

<pre><code>scrapy runspider spider.py -o outputfile.csv -t csv
</code></pre>

<p>(or the <code>crawl</code> command)</p>

<p>you would have the following in the <code>outputfile.csv</code>:</p>

<pre><code>Name,Year
"Faith, Love &amp; Chocolate",2018
Bennett's Song,2018
...
Tender Mercies,1983
You Might Be the Killer,2018
</code></pre>
    </div>