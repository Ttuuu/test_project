<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>To answer your first question of why there was such a big speedup when comparing your first version to your updated version (which took inspiration from Alviy's solution), it's because the first version is doing an expensive lookup within a loop that runs many times:</p>

<pre class="lang-py prettyprint-override"><code># obstacles is List[List[int]]
if [r_q_temp,c_q_temp] in obstacles  # [...]
</code></pre>

<p>In the above snippet, we are doing an <span class="math-container">\$O(k)\$</span> search (where <span class="math-container">\$k\$</span> is the length of <code>obstacles</code>) for <code>[r_q_temp, c_q_temp]</code> in <code>obstacles</code>, and we're doing this every time we advance the queen by one space!</p>

<p>The updated version uses a dictionary <code>obstacle_dir</code>, where lookup operations are on average <span class="math-container">\$O(1)\$</span>, which makes a huge difference:</p>

<pre><code>v1: 3.901795560005121 seconds per run (total 5 runs)
v2: 0.07246560000348837 seconds per run (total 5 runs)
</code></pre>

<p><code>v1</code> is the first version, and <code>v2</code> is the updated version.</p>

<p>And actually, if we just make two small changes to your first version (let's call it <code>v1_tweaked</code>), it runs faster than your updated version:</p>

<ol>
<li><p>Instead of an obstacle dictionary, create an obstacle set of (row, column) tuples. Like dictionaries, sets in Python also have an average <span class="math-container">\$O(1)\$</span> lookup time. In this case, a set is the more appropriate data structure because we only need to test membership; we don't need to map the board coordinates to anything.</p>

<pre class="lang-py prettyprint-override"><code># Set[Tuple[int, int]]
obstacles = {tuple(obstacle) for obstacle in obstacles}
</code></pre></li>
<li><p>Checking if a space contains an obstacle now looks like this:</p>

<pre class="lang-py prettyprint-override"><code>if (r_q_temp, c_q_temp) in obstacles # [...]
</code></pre></li>
</ol>

<pre><code>v1: 3.901795560005121 seconds per run (total 5 runs)
v2: 0.07246560000348837 seconds per run (total 5 runs)
v1_tweaked: 0.06353590001817792 seconds per run (total 5 runs)
</code></pre>

<hr>

<p>EDIT: There appears to be some difference in the way you and I are timing the functions, because when I ran <code>v2</code> (the updated version), <code>v1_tweaked</code> (my adjusted version of your <code>v1</code>), and Arviy's version, both <code>v2</code> and <code>v1_tweaked</code> ran faster than Arviy's version:</p>

<pre class="lang-py prettyprint-override"><code>from functools import partial
import timeit

n, k = 100000, 100000
r_q, c_q = 1, 1
obstacles = [[1000, 1000], [714, 169], ...]

v2 = partial(queensAttack_v2, n, k, r_q, c_q, obstacles)
v1_tweaked = partial(queensAttack_v1_tweaked, n, k, r_q, c_q, obstacles)
f_arviy = partial(queensAttack_Arviy, n, k, r_q, c_q, obstacles)

def measure(f, fname, num_trials=1):
    t = timeit.timeit(f'{fname}()', setup=f'from __main__ import {fname}', number=num_trials)
    print(f'{fname}: {t / num_trials} seconds per run (total {num_trials} runs)')

if __name__ == '__main__':
    TRIALS = 50
    measure(v2, 'v2', num_trials=TRIALS)
    measure(v1_tweaked, 'v1_tweaked', num_trials=TRIALS)
    measure(f_arviy, 'f_arviy', num_trials=TRIALS)
</code></pre>

<pre><code>v2: 0.07151672399835661 seconds per run (total 50 runs)
v1_tweaked: 0.06288900200044736 seconds per run (total 50 runs)
f_arviy: 0.0828415279998444 seconds per run (total 50 runs)
</code></pre>

<p>I am using the <a href="https://docs.python.org/3.8/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code></a> module, which is the recommended way to measure execution time of code snippets (more accurate than saving the time with <code>time.time()</code> before and after and calculating the difference).</p>
    </div>