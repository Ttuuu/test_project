<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Your original code:</p>

<pre><code>with open(PR_out_path, 'a') as g:
    for name in PR_files:
        with open(PR_path + name, 'r') as f:
            g.write(f.read())
</code></pre>

<p>works but, as you found, has problems if the entire file can't be read into memory.  The solution to that problem is to read the input file in chunks:</p>

<pre><code>with open(PR_out_path, 'a') as g:
    for name in PR_files:
        with open(PR_path + name, 'r') as f:
            while True:
                data = f.read(ChunkSize)
                if not data:
                    break
                g.write(data)
</code></pre>

<p>where <code>ChunkSize</code> is something like 1GB.</p>

<p>But if speed is your only requirement why not use the tools offered by the operating system, as others have noted?</p>
    </div>