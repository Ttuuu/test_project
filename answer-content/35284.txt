<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I tried running your implementation on my machine but with the following input (input is 25 random integers within the interval <code>[0; 10^9]</code>) <code>Initialize</code> never completes (it's stuck in the bucket <code>while (flag)</code> loop):</p>
<pre><code>25
882868245
264589055
955665379
570725902
186426836
425509062
780811177
528755197
921593609
210302061
162860187
237314629
771563954
716724339
500613765
749586096
118952462
708453275
530816792
697958285
841037949
796725013
123270367
470484394
578476359
1
252678354
</code></pre>
<p>Note: Never in this context means I killed it after 10 or 15 seconds.</p>
<p>So it's not surprising that a list with 50,000 random elements can easily result in a higher setup time than 100,000 depending on the distribution of the values (the algorithm attempts to re-create the table every time there is a collision). As per given example it's easy to generate inputs which will make it run for a long time for very few values.</p>
<p>Given the problem restrictions I'd say you can get away with the fastest perfect hash function there is: Identity.</p>
<p>You could simply use a bitset with the values as indices and flip the bits on if the value is present. This requires a bitset with 10^9 bits - approx. 125MB of memory which is not all that much these days (at least on desktops).</p>
<p>Resulting implementation:</p>
<pre><code>class FixedSet
{
    vector&lt;bool&gt; _set;

public:
    FixedSet() : _set(1000000000) { }

    void Initialize(const vector&lt;int&gt;&amp; numbers)
    {  
        for (int i = 0; i &lt; numbers.size(); ++i)
        {
            _set[numbers[i]] = true;
        }
    }

    bool Contains(int number)
    {
        return _set[number];
    }
};
</code></pre>
<p><code>Initialize</code> takes approx 45ms on my machine - it doesn't matter if it's 1 or 100,000 input elements. Apparently allocating the memory for the bitset takes the bulk of the time. Flipping the bits on seems to hardly take any time at all.</p>
<p>Searching for 10,000 random elements takes about 3ms</p>
<p>Some notes:</p>
<ul>
<li>In case you are wondering: <a href="http://en.cppreference.com/w/cpp/container/vector_bool" rel="nofollow noreferrer"><code>std::vector&lt;bool&gt;</code></a> is a specialization of a vector packing bools as bits. This container is considered somewhat as a failure in the C++ standard but happens to do exactly what we need here.</li>
<li>This implementation of <code>FixedSet</code> has space complexity <code>O(1)</code> (albeit a fairly big constant)</li>
<li>Initializing it has time complexity <code>O(n)</code> although the difference between 1 and 100,000 input elements is not measurable with the provided timing method</li>
<li>Looking up an element has time complexity of <code>O(1)</code> (indexed container access)</li>
<li>Disadvantages:
<ul>
<li>Fairly big initial upfront cost for setup (memory allocation)</li>
<li>Might not have been in the spirit of the assignment</li>
</ul>
</li>
</ul>
<p><strong>Update</strong></p>
<p>I think the main problem with your implementation is that the bucket size is fairly small (from testing it looks like you rarely get more than 6 or 7 collisions). Especially in buckets with more than 4 collisions your hash function actually throws away all hashes larger than 17 (which is <code>p_prime</code> you pass in from bucket) so you reduce your available bucket space. Also my suspicion is that in your hash function <code>a_prime</code> and <code>b_prime</code> should actually be, well, prime but <code>rand() % prime</code> does not yield a prime number.</p>
<p>While reading up on hashing, a popular version seems to be <a href="http://en.wikipedia.org/wiki/Cuckoo_hashing" rel="nofollow noreferrer">Cuckoo Hashing</a> which has a constant look up time (if you use two hash functions it will perform at most two lookups) and also a constant worst case insert time for an element (even if you have to rehash).</p>
<p>I threw together a quick hack implementation which does not do re-hashing and just relies on the sets being big enough:</p>
<pre><code>class FixedSet
{
    // min 36500, sweetspot 73000
    static const int _size = 73000;
    int _noentry;
    std::vector&lt;int&gt; _set1;
    std::vector&lt;int&gt; _set2;
    std::vector&lt;int&gt; _set3;

public:
    FixedSet() : _noentry(std::numeric_limits&lt;int&gt;::min()), _set1(_size, _noentry), _set2(_size, _noentry), _set3(_size, _noentry)  {  }

    void Initialize(const vector&lt;int&gt;&amp; numbers)
    {  
        for (int i = 0; i &lt; numbers.size(); ++i)
        {
            if (!add(numbers[i]))
            {
                std::ostringstream o;
                o &lt;&lt; "Failed to insert after " &lt;&lt; i &lt;&lt; " elements, rehashing not implemented";
                throw new std::exception(o.str().c_str());
            }
        }
    }

    bool Contains(int number)
    {
        for (int round = 0; round &lt; 3; ++round)
        {
            std::vector&lt;int&gt;&amp; _set = (round % 3 == 0) ? _set1 : ((round % 3 == 1) ? _set2 : _set3);
            int h = hash(round + 1, number);
            if (number == _set[h])
                return true;
        }
        return false;
    }

private:
    int hash(int rounds, int number)
    {
        int withOffset = number;
        srand(withOffset);
        int h = bigRand() % _size;
        while (--rounds)
        {
            h = bigRand() % _size;
        }
        return h;
    }

    inline int bigRand()
    {
        return (rand() &lt;&lt; 15) | rand(); // RAND_MAX is 0x7FFF in VS2010
    }

    bool add(int number)
    {
        int toInsert = number;
        for (int i = 0; i &lt; _size; ++i)
        {
            int h = hash(i % 3 + 1, toInsert);
            std::vector&lt;int&gt;&amp; _set = (i % 3 == 0) ? _set1 : ((i % 3 == 1) ? _set2 : _set3);
            int current = _set[h];
            if (current == _noentry)
            {
                _set[h] = toInsert;
                return true;
            }
            _set[h] = toInsert;
            toInsert = current;
        }
        return false;
    }
};
</code></pre>
<p>Notes:</p>
<ul>
<li>I'm using 3 hash functions instead of the classic 2. Can't get the load factor high enough with just 2 functions for some reason.</li>
<li>The above is tweaked for no more than 100,000 input elements</li>
<li><code>36500</code> seems to be the minimum set size which yields a 91% load factor of the sets which is pretty much the limit for Cuckoo with 3 hash functions according to wikipedia.</li>
<li>With <code>36500</code> set size it takes approx. 74ms on my machine to insert 100,000 elements.</li>
<li>Doubling to <code>73000</code> yields a 75% reduction in run time to 17ms.</li>
<li>Further increases in the set size do not seem to have much effect.</li>
</ul>
    </div>