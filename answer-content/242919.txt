<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>This</p>

<pre><code># Define the Rosenbrock Function
def f(x_k):
    x, y = x_k[0, 0], x_k[0, 1] 
    return 100 * (y - x**2)**2 + (1 - x)**2
</code></pre>

<p>could be</p>

<pre><code>def f_rosenbrock(xy):
    x, y = xy
    return 100 * (y - x**2)**2 + (1 - x)**2
</code></pre>

<p>This</p>

<pre><code># Gradient of f 
def gradient(x_k):
    x, y = x_k[0, 0], x_k[0, 1] 
    return  np.array([-400*x*(y-x**2)-2*(1-x), 200*(y-x**2)])
</code></pre>

<p>could be</p>

<pre><code>def df_rosenbrock(xy):
    x, y = xy
    return  np.array([-400*x*(y-x**2)-2*(1-x), 200*(y-x**2)])
</code></pre>

<p>It wouldn't cost much to turn <code>main</code> into a more general gradient descent function having the following signature:</p>

<pre><code>def gradient_descent(f, d_f, x0):
    # Define the starting guess
    x_k = x0
    # ...
</code></pre>

<p>You could add the following condition so that this code won't run if imported as a module.</p>

<pre><code>if __name__ == '__main__':
    # main()
    gradient_descent(f_rosenbrock, df_rosenbrock, np.array([10, 5]))
</code></pre>

<p>It'd probably be the best to stick to either <code>camelCase</code> or <code>snake_case</code> for variable names. The second is more popular. E.g. <code>num_steps</code> instead of <code>numSteps</code>.</p>

<p>Don't evaluate the gradient so many times:</p>

<pre><code>    while abs((gradient(x_k)[0, 0])) &gt; 0.1 or abs((gradient(x_k))[0, 1]) &gt; 0.1:
        # ...
        p_k = - gradient(x_k)
        gradTrans = - p_k.T

        # ...
    print("The gradient is: ", gradient(x_k))
</code></pre>

<p>could be</p>

<pre><code>    while True:
        g_k = df(x_k)

        if np.abs(g_k).max() &lt; tol:
            break    
    # ...
    print("The gradient is: ", g_k)
</code></pre>

<p>We don't need <code>gradTrans</code>, nor <code>p_k</code>.</p>

<p>This</p>

<pre><code>    # Now we use a backtracking algorithm to find a step length
    alpha = 1.0
    ratio = 0.8
    c = 0.01 # This is just a constant that is used in the algorithm

    # This loop selects an alpha which satisfies the Armijo condition  
    while f(x_k + alpha * p_k) &gt; f(x_k) + (alpha * c * (gradTrans  @ p_k))[0, 0]:
        alpha = ratio * alpha

    x_k = x_k + alpha * p_k
</code></pre>

<p>part is probably the worst offender wrt. performance. You don't have to recalculate all of these values. Some constants are hardcoded, while they could easily become parameters.</p>

<p>Anyway, putting it all together we get something like the following.
Feel free to add comments to it, but use docstrings whenever appropriate.</p>

<pre><code>import numpy as np

def f_rosenbrock(xy):
    x, y = xy
    return 100 * (y - x**2)**2 + (1 - x)**2

def df_rosenbrock(xy):
    x, y = xy
    return np.array([-400*x*(y-x**2)-2*(1-x), 200*(y-x**2)])

def gradient_descent(f, df, x0, tol=.1, alpha=1.0, ratio=.8, c=.01):
    x_k, num_steps, step_size = x0, 0, alpha
    while True:
        g_k = df(x_k)

        if np.abs(g_k).max() &lt; tol:
            break

        num_steps += 1

        fx, cg = f(x_k), - c * (g_k**2).sum()
        while f(x_k - step_size * g_k) &gt; fx + step_size * cg:
            step_size *= ratio

        x_k -= step_size * g_k

    return x_k, g_k, num_steps

if __name__ == '__main__':
    x, g, n = gradient_descent(
        f_rosenbrock, df_rosenbrock, np.array([10.0, 5.0])
    )
    print("The number of steps is: ", n)
    print("The final step is:", x)
    print("The gradient is: ", g)
</code></pre>
    </div>