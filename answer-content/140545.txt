<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<pre><code>import numpy as np

class kmeans():
</code></pre>

<p>Use a PEP 8 checker such as pycodestyle or flake8. Integrate it into your file editor or IDE, if you're not doing it already. Some violations get missed by those tools. For example, class names should use uppercase, so "KMeans" here.</p>

<pre><code>    '''
    Implementation of classical k-means clustering algorithm
    parameters : dataset n x m ndarray of n samples and m features
    n_clusters : number of clusters to assign samples to
    limit : tolerance between successive iterations
    '''
    def __init__(self, dataset, n_clusters, limit):

        self.dataset = dataset
        self.n_clusters = n_clusters
        self.limit = limit

        # dictionary to hold each cluster as a list of samples
        self.clusters = {i: [] for i in range(self.n_clusters)}
</code></pre>

<p>For speed, consider storing the cluster membership information in the dataset.</p>

<pre><code>        # the centroids of each cluster
        self.centroids = np.ndarray((n_clusters, dataset.shape[1]))
</code></pre>

<p>I like that the code is generic enough to handle more than two dimensions. üëç</p>

<pre><code>        # values of utility function. increases in size  by 1
        # in each iteration
        self.util_func_vals = []
</code></pre>

<p><code>util_func_vals</code> is not really descriptive. First, we <a href="https://en.wikipedia.org/wiki/K-means_clustering" rel="nofollow noreferrer">usually describe this as the objective</a>, not as an "utility function". And <code>vals</code> does not add value to the name. Consider using a name such as <code>objective_history</code>. Not great, but better. Finding great names is hard, but it's worth it.</p>

<pre><code>    def assign_to_clusters(self):

        for idx, sample in enumerate(self.dataset):
</code></pre>

<p>A for loop! This is among the first thing you should try to eliminate to make your code faster. This is called vectorizing your code. You can probably make most operations on the whole <code>self.dataset</code> at once. See this <a href="https://codereview.stackexchange.com/a/63245/11227">awesome k-means answer</a> to see how it could be done.</p>

<pre><code>            distances = []
            # for each sample we compute its distance from every centroid
            for centroid in self.centroids:
                distances.append(np.linalg.norm(sample - centroid))
</code></pre>

<p>For speed, don't use linalg.norm, because it does an extra square root that you don't need. Indeed sqrt(a) &gt; sqrt(b) is equivalent to a &gt; b, so you don't need the square root. However, I tried to replace linalg.norm with a version without square roots, and it was slower. Always profile your code before trying to optimize it.</p>

<pre><code>    def calc_utility_function(self):

        total_sum = 0
        # utility function is the sum of intra-cluster distances
        # the goal is to minimize it
        for cluster, samples in self.clusters.items():

            for i in range(len(samples)):

                for j in range(i + 1, len(samples)):

                    total_sum += np.linalg.norm(samples[i] - samples[j])

        return total_sum
</code></pre>

<p>I profiled your code using cProfile and gprof2dot, and this takes 85% of the time.</p>

<p><a href="https://i.stack.imgur.com/FMCqr.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/FMCqr.png" alt="Profiling"></a></p>

<p>85%! But it only serves to know when to stop, this should not be the most expensive function. And indeed, there's a mistake. You're computing the distance between all points in a given cluster, which is O(n^2), while you only need to compute the distance for all points to the centroid, which is O(n), much much faster. Here's the new function:</p>

<pre><code>def calc_utility_function(self):
    total_sum = 0
    for cluster, samples in self.clusters.items():
        centroid = self.centroids[cluster]
        for i in range(len(samples)):
            total_sum += np.linalg.norm(samples[i] - centroid)

    return total_sum
</code></pre>

<p>If it took 1s for 100 samples, it will now take about 4s for 400 samples. But with your code it would have taken 16s. For 200 samples, it's a 10x improvement. The <code>calc_utility_function</code> optimization was so effective that most of the time is spent initializing the code. I went to 2000 samples and profiled again:</p>

<p><a href="https://i.stack.imgur.com/P5mqv.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/P5mqv.png" alt="Profiling again"></a></p>

<p>The function that was previously taking 85% of the time is now taking 15% of the time.</p>

<pre><code>    def calc_new_centroids(self):
        # we calculate new centroids by obtaining the centers of each
        # (each) cluster
</code></pre>

<p>Not much to say here, but try to see if this can be vectorized.</p>

<pre><code>        # clusters dictionary must empty in order to repopulate
        self.clusters = {i: [] for i in range(self.n_clusters)}
</code></pre>

<p>Don't do this here, but do it at the beginning of the function. This way, it's easy to access the dictionary outside of the function since there's no side effect.</p>

<pre><code>    def compute(self):
        # core method that computes the clusters
        # initialize centroids by randomly choosing #n_clusters samples 
        # from dataset 
        self.centroids = self.dataset[np.random.choice(self.dataset.shape[0],
                                                   size=self.n_clusters,
                                                   replace=False), :]

        # apply the first two steps of the algorithm
        self.assign_to_clusters()
        self.util_func_vals.append(self.calc_utility_function())

        self.calc_new_centroids()

        self.assign_to_clusters()
        self.util_func_vals.append(self.calc_utility_function())
</code></pre>

<p>This is ugly because you're repeating yourself. If you define self.utils_func_vals to [math.inf], then you can put everything in the loop (see below). </p>

<pre><code>        # and continue until the succesive value difference of utility
        # function becomes lower than the user specified limit
        while abs(self.util_func_vals[-1] - self.util_func_vals[-2]) &gt; self.limit:
</code></pre>

<p>Use <a href="https://docs.python.org/3/library/math.html#math.isclose" rel="nofollow noreferrer">math.isclose</a>: <code>math.isclose(self.util_func_vals[-1], self.util_func_vals[-2], abs_tol=limit)</code>.</p>

<p>Here's the new loop:</p>

<pre><code>    converged = False
    while not converged:
        self.assign_to_clusters()
        self.calc_new_centroids()
        self.util_func_vals.append(self.calc_utility_function())
        converged = math.isclose(
            self.util_func_vals[-1],
            self.util_func_vals[-2],
            abs_tol=self.limit)
</code></pre>

<p>Much cleaner, and there's no messy initialization.</p>

<p>And of course, if you really want speed, use the implementation from scipy as mentioned by @GarethRees.</p>
    </div>