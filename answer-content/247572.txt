<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I agree with your analysis that both algorithms have O(n) time complexity (where n is the number of elements in the array). An important factor to note is that both algorithms also use constant extra space.</p>
<p>However, you can include other metrics in your analysis too. Anything that may be expensive  to do can be considered. Two common metrics are the number of comparisons and the number of exchanges.</p>
<p>The number of comparisons tells you how often an element of the array is accessed and then compared. Memory access can be quite expensive, depending on how far up the memory hierarchy (registers, cache levels, RAM, local storage, external storage) you need to traverse before getting the value.</p>
<p>The number of exchanges tells you how many times elements are swapped. This can be expensive any time writing to memory is. For instance, writing to shared memory in a multi-process program.</p>
<hr>
<p>Taking the above into account, ignoring the error checking case, and also ignoring any potential optimisations from the compiler, I would argue the second algorithm is better. For the first implementation I count 2 comparisons per element, and 1 exchange per element. For the second, I count 1 comparison per element, and then 1 assignment per element (an assignment of a constant is cheaper than exchanging two elements).</p>
<p>You can replace the exchange with assignments in the first implementation. Since the array only contains the values 0 and 1, and you've checked that the two elements are different.</p>
<pre><code>if (j &gt; i &amp;&amp; value[i] &gt; value[j] )
{
    value[j] = 1;
    value[i] = 0;
    j--;
    i++;
}
</code></pre>
<p>Without profiling the code, I don't think it is possible to tell if the extra number of comparisons in the first implementation are more costly than the overhead of the second pass through the array in the second implementation. I would not be surprised if the numerous bounds checks in the first piece of code (<code>i &lt; j</code>) are the actually largest performance hit.</p>
<hr>
<h2>Third implementation</h2>
<p>Based on the comment that the first implementation is still faster, here is another solution that tries to improve upon the second. I'm hoping that by writing it with well known operations, the compiler can work some magic.</p>
<pre><code>public static int[] SortOnesZerosAlternate(int[] values)
{
    int countOfZeros = values.Length - values.sum();
    // Maybe replace this with values.Clear?
    for (int i = 0; i &lt; countOfZeros; i++)
    {
        values[i] = 0;
    }

    for (int i = countOfZeros; i &lt; values.Length; i++)
    {
        values[i] = 1;
    }

    return values;
}
</code></pre>
    </div>