<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>in theory you can measure entropy only from the point of view of a given model. For instance the PI digits are well distributed, but actually is the entropy high? Not at all since the infinite sequence can be compressed into a small program calculating all the digits.</p>

<p>I'll not dig further in the math side, since I'm not an expert in the field. But I want to suggest to you a few things that can make a very simple but practical model.</p>

<p><strong>Short strings</strong></p>

<p>To start, distribution. Comparing characters that are the same is exactly this in some way, but the generalization is to build a frequency table and check the distribution.</p>

<p>Given a string of length N, how many A chars should I expect in average, given my model (that can be the english distribution, or natural distribution)?</p>

<p>But then what about "abcdefg"? No repetition here, but this is not random at all.
So what you want here is to take also the first derivative, and check the distribution of the first derivative.</p>

<p>it is as trivial as subtracting the second char from the first, the thrid from the second, so in our example string this turns into: "abcdefg" =&gt; 1,1,1,1,1,1</p>

<p>Now what aobut "ababab"... ? this will appear to have a better distribution, since the derivative is 1,-1,1,-1,... so what you actually want here is to take the absolute value.</p>

<p><strong>Long strings</strong></p>

<p>If the string is long enough the no brainer approach is: try to compress it, and calculate the ratio between the compression output and the input.</p>
    </div>