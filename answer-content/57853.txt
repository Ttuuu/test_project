<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h1><strong>Wrong Approach</strong></h1>
<p>I think what you are trying to do makes sense, however I think that your row-by-row approach is what is making this so slow. Anything you do row-by-row with SQL (e.g., a cursor or loop) will be slow because you are executing the whole section of query anew for each row. All SQL is optimized to work with large data sets, not single rows.</p>
<h1><strong>Suggested Approach</strong></h1>
<p>I feel sure that you could establish a connection so the BigQuery server directly and pass it sets of data via <a href="http://msdn.microsoft.com/en-us/library/ms190479.aspx" rel="nofollow noreferrer">sp_addlinkedserver</a> or similar approach. Contact the people at BigQuery to help with this.</p>
<p>Do scan through your files to insert the data into your local SQL server. Then you could just do something like:</p>
<pre><code>INSERT INTO [BigQueryServer].[database].[schema].[table]
    SELECT * FROM [LocalServer].[database].[schema].[table]
    WHERE [LocalServer].[database].[schema].[table].[added_timestamp] -- or whatever column you use to keep track of records added
        &gt;= '2014-07-01' -- or whatever date
</code></pre>
    </div>