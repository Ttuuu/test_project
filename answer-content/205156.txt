<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>The code looks good, I wouldn't suggest anything as needing to be changed. In line with your question about possible ways to use Numpy better, I'll offer a few things you could try:</p>

<ul>
<li><p>You can pass low/high bounding arrays into <code>numpy.randint</code>, like <code>.randint(low=data_min, high=data_max, size=(k, n_dimensions)</code>, rather than adjust the dimension-specific bounds after the fact. This is only done once, so it doesn't matter w.r.t. performance, but it cleans the code up a little.</p></li>
<li><p>You <em>can</em> pack the distance computations into purely numpy code, but I don't know if I'd recommend it. Generally, K is very small compared to the size of your data, so your list comprehension is extremely cheap and readable. The alternative numpy code would abuse broadcasting to trick numpy into doing the looping for you, and would look something like this:</p>

<pre><code>distances = np.linalg.norm(np.expand_dims(d, 2) - np.expand_dims(centroids.T, 0), axis=1)
new_labels = distances.argmin(axis=1)
</code></pre>

<p>I wouldn't suggest using that code, though, since its performance gains are likely minimal (you can check for your own use case) and it's much less readable than what you currently have.</p></li>
<li><p>You're right to be concerned about the centroid computation, since your current code is effectively O(K * N), whereas there's clearly a solution that's just O(N). The challenge, however, is to do so without losing the simplicity/readability of your code combined with numpy's inherent speed. <a href="https://stackoverflow.com/questions/4373631/sum-array-by-number-in-numpy">Here's an interesting discussion about techniques to do a similar problem</a>. Of those approaches, my quick tests say that the <code>np.bincount</code> method is fastest, but it doesn't work with multi-dimensional "weights", which is essentially what we need. My only thought that <em>might</em> be faster than your current approach is to write the code simply, then use Numba to JIT compile it to get C-level performance. That code would look like:</p>

<pre><code>@numba.jit
def mean_vecs_by_group(mat, groups, num_groups=None):
    sum_vecs = np.zeros(shape=(num_groups, mat.shape[1]))
    counts = np.zeros(shape=(num_groups,))
    for g, vec in zip(groups, mat):
        sum_vecs[g] += vec
        counts[g] += 1
    return sum_vecs / np.expand_dims(counts, -1)
</code></pre>

<p>However, in my test runs, this performs 1.5-10x slower than your current numpy code. I don't know of any other way in Numpy to do this all in one iteration through the array (if someone else knows, please comment). I suspect that in most cases, your current code is both the most maintainable solution and has very competitive performance.</p></li>
</ul>
    </div>