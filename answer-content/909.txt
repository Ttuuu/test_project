<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I also came up with this, based on <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Shannon entropy</a>.</p>

<blockquote>
  <p>In information theory, entropy is a measure of the uncertainty associated with a random variable. In this context, the term usually refers to the <strong>Shannon entropy, which quantifies the expected value of the information contained in a message, usually in units such as bits.</strong> </p>
</blockquote>

<p>It is a more "formal" calculation of entropy than simply counting letters:</p>

<pre><code>/// &lt;summary&gt;
/// returns bits of entropy represented in a given string, per 
/// http://en.wikipedia.org/wiki/Entropy_(information_theory) 
/// &lt;/summary&gt;
public static double ShannonEntropy(string s)
{
    var map = new Dictionary&lt;char, int&gt;();
    foreach (char c in s)
    {
        if (!map.ContainsKey(c))
            map.Add(c, 1);
        else
            map[c] += 1;
    }

    double result = 0.0;
    int len = s.Length;
    foreach (var item in map)
    {
        var frequency = (double)item.Value / len;
        result -= frequency * (Math.Log(frequency) / Math.Log(2));
    }

    return result;
}
</code></pre>

<p>Results are:</p>

<pre>
"abcdefghijklmnop" = 4.00
"Hello, World!" = 3.18
"hello world" = 2.85
"123123123123" = 1.58
"aaaa" = 0
</pre>
    </div>