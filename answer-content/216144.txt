<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Some quick suggestions: </p>

<p>The <code>requests</code> module can urlencode strings for you if you use the <code>params</code> keyword:</p>

<pre><code>import requests

cities = ["Menlo Park, CA"]
pages = range(1, 3)
url = "https://www.facebook.com/careers/jobs/"

for city in cities:
    for page in pages:
        params = {"page": page, "results_per_page": 100, "locations[0]": city}
        response = requests.get(url, params=params)
</code></pre>

<p>Organize your code using functions. This allows you to give them a readable name (and even add docstrings).</p>

<pre><code>def get_job_infos(response):
    """Parse the content of the request to get all job postings"""
    page_soup = BeautifulSoup(response.text, 'lxml')
    job_containers = page_soup.find_all("a", "_69jm")

    # Select all 100 jobs containers from a single page
    for container in job_containers:
        site = page_soup.find("title").text
        title = container.find("div", "_69jo").text
        location = container.find("div", "_1n-z _6hy- _21-h").text
        job_link = "https://www.facebook.com" + container.get("href")
        yield site, title, location, job_link
</code></pre>

<p>This is a generator over which you can iterate. Using the <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser" rel="nofollow noreferrer"><code>lxml</code> parser</a> is usually faster.</p>

<p>Note that <code>csv</code> can write multiple rows at once using <a href="https://docs.python.org/3/library/csv.html#csv.csvwriter.writerows" rel="nofollow noreferrer"><code>writer.writerows</code></a>, which takes any iterable of rows:</p>

<pre><code>with open('facebook_job_list.csv', 'a', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(get_job_infos(response))
</code></pre>

<p>This way you only have to open the file once per page, instead of a hundred times. Even better would be to make the whole thing a generator, so you can write all rows while opening the file only once:</p>

<pre><code>def get_all_jobs(url, cities, pages):
    for city in cities:
        for page in pages:
            params = {"page": page, "results_per_page": 100, "locations[0]": city}
            response = requests.get(url, params=params)
            # check status code

            yield from get_job_infos(response)

            # rate throttling, etc here
            ...

if __name__ == "__main__":
    cities = ["Menlo Park, CA", ...]
    pages = range(1, 3)
    url = "https://www.facebook.com/careers/jobs/"

    with open('facebook_job_list.csv', "w") as f:
        writer = csv.writer(f)
        writer.writerow(["Website", "Title", "Location", "Job URL"])
        writer.writerows(get_all_jobs(url, pages, cities))
</code></pre>

<p>This way the <code>get_all_jobs</code> generator will <code>yield</code> jobs as it is being iterated over, getting the next page when needed.    </p>
    </div>