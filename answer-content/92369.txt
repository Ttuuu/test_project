<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>How long does your code take to find the first million primes (that is, all the primes below 15,485,864)?</p>

<pre><code>&gt;&gt;&gt; from timeit import timeit
&gt;&gt;&gt; timeit(lambda:list(islice(primes(), 10**6)), number=1)
5.053490979000344
</code></pre>

<p>Compared to the implementations in <a href="https://codereview.stackexchange.com/a/42439/11728">this answer</a>, this is faster than <code>sieve</code> but slower than <code>sieve2</code>:</p>

<pre><code>&gt;&gt;&gt; timeit(lambda:list(sieve(15485864)), number=1)
10.124665475999791
&gt;&gt;&gt; timeit(lambda:list(sieve2(15485864)), number=1)
3.4201999040014925
</code></pre>

<p>Even though these implementations ought to be asympotically worse.</p>

<p>The trouble with implementing this kind of algorithm in Python is that the big overhead of the Python interpreter tends to overwhelm small algorithmic improvements, if those improvements mean that you end up spending more time in the slow Python bytecode and less time in the fast C implementation.</p>

<h3>Update</h3>

<p>There's some confusion in comments about what I mean by "asympotically worse" above. The complexity of sieving for primes below \$n\$ is $$ C n \log \log n + O(1) $$ for some constant \$C\$. Wheel sieving using the first \$k\$ primes theoretically saves a factor of $$ \prod_{i\le k}{p_i-1\over p_i},$$ so wheel sieving with 2 should halve the runtime; with 2 and 3 it should reduce the runtime to a third, and so on. This is a constant factor of improvement, so it wouldn't appear in the big-O analysis (it would get folded into the constant \$C\$). But in real life, constant factors matter too: effectively in your code you are exchanging a larger number of cheap operations for a smaller number of expensive operations. With a large-enough wheel, you ought to be able to beat the na√Øve algorithms. But it might have to be quite large.</p>
    </div>