<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p><strong>TLDR</strong>: it can be done much more quickly and much more concisely, in <strong>28 minutes</strong> end-to-end, and in <strong>12 minutes</strong> when all XML are already on disk. The key trick: using the <strong>long</strong> rather than <strong>wide</strong> format for each data frame.</p>

<p>There are two parts to your code: style and performance. Let me post my approach and comment on why I coded it like this.</p>

<p>For imports, I separate Python standard libraries and user-installed libraries, and put them both in alphabetical order. This makes your code more readable.</p>

<pre><code>import os
import zipfile

import click
from lxml import etree
import pandas as pd
import requests
</code></pre>

<p>You didn't post it, so I had to write my own code to download and unzip all the CDC XML files:</p>

<pre><code>def download_data(url, file):
    response = requests.get(url, stream=True)
    assert response.status_code == 200
    with open(file, 'wb') as dst:
        for chunk in response.iter_content(chunk_size=4096):
            dst.write(chunk)

def unzip_data(file, path):
    with zipfile.ZipFile(file) as src:
        src.extractall(path)

url = 'https://clinicaltrials.gov/AllPublicXML.zip'
file = url.split('/')[-1]
path = file.split('.')[0]

download_data(url, file)   
unzip_data(file, path)
</code></pre>

<p>Note that I didn't write out my full path for my working directory, this is all implicitly handled relative to the working directory from where you run your code (yeah, explicit is better than implicit, but in this case I ignore the Zen of Python).</p>

<p>Next, there is the issue of parsing a single XML file. Your approach used two iterations over the parse tree, and some unnecessary lambdas stored in variables (why not do it in place?). It is possible to iterate just once over the parse tree, and extract both the xpath and the text. Furthermore, it will turn out that storing this in <strong>long format</strong> as a 3-column data frame with a variable number of rows will gain an order of magnitude when concatenating these data frames later on. </p>

<pre><code>def parse_xml(file):
    # The CDC XML files are named NCTyyyyxxxx, this extracts the 8 yyyyxxxx digits
    id = int(os.path.splitext(os.path.basename(file))[0][-8:])
    tree = etree.parse(file)
    return pd.DataFrame(
        data=[
            (id, tree.getpath(elem), elem.text)
            for elem in tree.iter()
        ],
        columns=['id', 'key', 'value']
    )
</code></pre>

<p>Note that there is no longer a need to have an intermediate dict. The code above just parses the tree into a list of tuples and initializes a data frame with it.</p>

<p>Next, the iteration over the directory tree is most conveniently done using the standard library, in particular <code>os.walk</code>. Note that in your code, you not only had to manually check for directory and files, but you also manually combined directory names, file names and extensions rather than with the standard <code>os.path.join</code> (which would have made it platform independent).</p>

<pre><code>xml_files = [
    os.path.join(dirpath, file)
    for dirpath, _, filenames in os.walk(path)
    for file in filenames
    if file.endswith('.xml')
]
</code></pre>

<p>The code above stores the 335K XML file paths in a list (takes less than 1 second).</p>

<p>Finally, combining the above code is simply a loop over the files, parsing each file and concatenating the results. I use the progressbar to see how fast it will be, but in this case the time is a mere <strong>12 minutes</strong>.</p>

<pre><code>with click.progressbar(xml_files) as bar:
    df = pd.concat((
        parse_xml(f)
        for f in bar
    ))
df.to_csv('output.csv')
</code></pre>

<p>Inspecting this data frame shows that it has almost 100M rows and 3 columns. In contrast, the wide format would have had 335K rows but 730K columns. This is the reason that this code ran so slow: concatenating / appending all these differently laid-out data frames requires an inordinate amount of data copying to align it in the final data frame. In contrast, the long format just appends 3 known columns. </p>

<p>Total file on disk is 11 Gb.</p>

<p><strong>EDIT</strong>: in the end I didn't manage to run your posted code because of memory overflow (335K rows x 730K columns will do that unless you have a Tb of RAM). Note that I didn't use a global variable to which I <code>append()</code> each parsed XML file. The Pandas docs state that <code>pd.concat()</code> should be more performant. It was for my approach: the progress bar indicated that appending to an initially empty data frame would increase the total time to over <strong>2 hours</strong>.</p>
    </div>