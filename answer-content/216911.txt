<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>This one is <code>O(1)</code> in terms of pyspark collect operations instead of previous answers, both of which are <code>O(n)</code>, where <code>n = len(input_df.columns)</code>. </p>

<pre><code>def get_binary_cols(input_file: pyspark.sql.DataFrame) -&gt; List[str]:
    distinct = input_file.select(*[collect_set(c).alias(c) for c in input_file.columns]).take(1)[0]
    print(distinct)
    print({c: distinct[c] for c in input_file.columns})
    binary_columns = [c for c in input_file.columns
                      if len(distinct[c]) == 2
                      and (set(distinct[c]).issubset({'1', '0'}) or set(distinct[c]).issubset({1, 0}))]
    return binary_columns
</code></pre>

<p>For ~100 columns and 100 rows I've gained around 80x boost in performance. </p>
    </div>