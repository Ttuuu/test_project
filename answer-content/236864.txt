<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>might have gone a bit far with the microbenchmarking here, but I've stripped out the irrelevant parts of your above functions, giving:</p>

<pre><code>def only_dict(d, i):
    d[i] = 0
    return d[i]

def without_walrus(d, i):
    r = 0
    d[i] = r
    return r

def with_walrus(d, i):
    d[i] = (r := 0)
    return r
</code></pre>

<p>i.e. just write the number zero into the dictionary instead of complicating things with also running <code>sum(range(10))</code>.  note that as soon as your code is doing anything as complicated as <code>sum(range(10))</code> (i.e. almost certainly) then the time of that will dominate and all of this doesn't matter</p>

<p>I've also written a special version which appears below as <code>patched_walrus</code> which is like <code>with_walrus</code> eliminates the store to <code>r</code>.  it's similar to:</p>

<pre><code>def patched_walrus(d, i):
    return (d[i] := 0)
</code></pre>

<p>AFAIK this can't be expressed in Python code, but the bytecode allows it and it was an interesting for me to include</p>

<p>it's important to reduce variance as much as possible, and because the code we're benchmarking is so small I'm using <a href="https://docs.python.org/3/library/timeit.html#timeit.Timer" rel="noreferrer"><code>Timer</code>s</a> directly as:</p>

<pre><code>from timeit import Timer

functions = [only_dict, without_walrus, with_walrus, patched_walrus]
timers = [
    Timer('fn(d, 0)', 'fn = func; d = {}', globals=dict(func=fn))
    for fn in functions
]

out = [
    tuple(t.timeit() for t in timers) for _ in range(101)
]
</code></pre>

<p>I ignore the first few runs as these tend to be slower due to various things like warming up the cache, e.g. your first two runs are noticeably slower because of this.  using <code>Timer</code> directly helps because it will compile the code once (rather than every time you call <code>timeit</code> and then the compiled code can remain hot in the cache.</p>

<p>next we can plot these in order:</p>

<p><a href="https://i.stack.imgur.com/z2qYz.png" rel="noreferrer"><img src="https://i.stack.imgur.com/z2qYz.png" alt="idx vs walrus"></a></p>

<p>which helps to see if your machine was busy as this can bias the results.  I've drawn outliers as dots and connected the rest.  the small plot on the right has <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" rel="noreferrer">KDE</a>s of the non-outlier distributions.</p>

<p>we can see that:</p>

<ol>
<li><code>only_dict</code> is about 10 nanoseconds per invocation faster, i.e. a tiny difference but we can reliably measure it now</li>
<li><code>without_walrus</code> and <code>with_walrus</code> are still basically the same</li>
<li>my special <code>patched_walrus</code> is a measurable 2 nanoseconds faster, but so fiddly to create it's almost certainly not worth it.  you'd be better writing a CPython extension module directly if you really care about performance</li>
</ol>
    </div>