<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Wikipedia contains circular references. Currently you follow them around until your max depth is reached. Instead, keep a <code>set</code> of all sites visited so far.</p>

<p>You also don't need this function to be recursive, you can make it iterative like this:</p>

<pre><code>from collections import deque

def build_link_graph(start, max_pages=10):
    queue = deque([start])
    visited = set()
    i = 0
    while queue and i &lt; max_pages:
        next_url = queue.popleft()
        urls = get_links(next_url)
        i += 1
        # make sure not to visit any page twice
        queue.extend(url for url in urls if url not in visited)  
        visited.update(urls)
        yield next_url, urls
</code></pre>

<p>The actual parsing can also be simplified using the <a href="https://www.w3schools.com/cssref/sel_element_gt.asp" rel="nofollow noreferrer">CSS selector <code>p &gt; a</code></a>, meaning all <code>a</code> tags which are contained in a <code>p</code> tag. I would also use a <a href="https://2.python-requests.org/en/master/user/advanced/#session-objects" rel="nofollow noreferrer"><code>requests.Session</code></a> to keep the connection alive and use the <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser" rel="nofollow noreferrer"><code>lxml</code> parser</a>, both of which slightly speed up the scraping.</p>

<pre><code>def filter_links(links):
    blacklist = ['#cite', '/wiki/Help', '/wiki///en:', '/wiki/Wikipedia']                                   
    return [link for link in links                    
            if not any(link.startswith(prefix) for prefix in blacklist)]

def get_links(url):
    page = SESSION.get(url)
    soup = BeautifulSoup(page.text, "lxml")
    links = [a["href"]
             for a in soup.select("p &gt; a")
             if "selflink" not in a.get("class", [])]
    return [BASE_URL + link for link in filter_links(links)]
</code></pre>

<p>With these functions, building the graph is rather easy. Note that you might want to choose a directed graph, since links are usually unidirectional. Also, <code>networkx</code> automatically adds missing nodes when adding edges, so no need to do that.</p>

<pre><code>import sys

if __name__ == "__main__":
    BASE_URL = "'https://en.wikipedia.org'"
    if len(sys.argv) == 2:
        start = sys.argv[1]
    else:
        start = BASE_URL + '/wiki/Earthquake_engineering'
    SESSION = requests.Session()
    g = nx.DiGraph()
    for url, urls in build_link_graph(start):
        graph.add_edges_from((url, link) for link in urls)
</code></pre>

<p>I added a <a href="http://stackoverflow.com/questions/419163/what-does-if-name-main-do"><code>if __name__ == "__main__":</code> guard</a> to allow importing from this script from another script without running the scraping</p>

<hr>

<p>Note that if you increase <code>max_pages</code> too much, you will run into a too many requests.response. Currently there is no code to handle that, but you might add some. The easiest way is to just <code>time.sleep</code> some time and try again afterwards. There are also other ways, some of which may not be in compliance with Wikipedia's ToS:</p>

<pre><code># robots.txt for Wikipedia and friends
# Please note: There are a lot of pages on this site, and there are
# some misbehaved spiders out there that go _way_ too fast. If you're
# irresponsible, your access to the site may be blocked.
</code></pre>

<p>There may be other ways to build this graph than scraping. Wikipedia supplies databases to download which should contain everything you need: <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Wikipedia:Database_download</a> Note that it is 58 GB when uncompressed, though.</p>
    </div>