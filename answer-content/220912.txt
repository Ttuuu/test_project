<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h2>Review</h2>
<p>At reading the problem I thought it needed to be a lossless compression algorithm to generate a unique id.</p>
<p>The most basic compression is <a href="https://en.wikipedia.org/wiki/Run-length_encoding" rel="nofollow noreferrer">run length encoding</a> on the bits that make the url. Or a dictionary compression on bit patterns such as <a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch" rel="nofollow noreferrer">Lempel, Ziv, Welch compression</a>  AKA LZW or LZ compression. However there is some storage overhead and for short URLs you will have trouble getting a smaller url than the original.</p>
<h2>Random id?</h2>
<p>Your solution stores the original url mapped to a random index. There is no need for a compression or even a hash as all you need is a unique id per URL.</p>
<p>To generate an unique id you need only the number line. JS <code>Number</code> can generate 9e15 unique ids, that's a new id every millisecond for the next quarter million years (twice that if you include negatives).</p>
<p>You can also compress the id by converting the id to base 36 (the max base for <code>Number.toString(radix)</code>) thus the longest encoding is <code>"2gosa7pa2gv"</code> and the shortest is <code>"0"</code> and you need store only one 64bit double for the short url.</p>
<p>See Example</p>
<h2>Bug?</h2>
<p>Your function returns a different short url for the same long url.</p>
<h2>Example</h2>
<ul>
<li><p>Simple index encoding, there is room for lots of optimization in storage.</p>
</li>
<li><p>Produces safe urls using only <code>a-z0-9</code> to encode.</p>
</li>
<li><p>There is a limit as it does rely on the hashing function (Chance of a clash) of the JS engine.</p>
</li>
<li><p>Checks if a url has already been encoded</p>
</li>
</ul>
<p>You can avoid the JS hash functions (and possible clash), by using an array or set of arrays (if number of urls high) but that comes with a huge CPU cost of locating the URL if the data set is big will require a linear search of the stored urls when encoding. Decoding will still be fast as the encoded url contains the index of the original url</p>
<pre><code>const PREFIX = 'http://tinyurl.com/';
const urls = {
    short: new Map(),  // can be array use id to index
    long: new Map(),   // can be array and use find rather than has and get
    value: 0,
    get id() { return urls.value++ },
};
function encode(url) {
    if (urls.long.has(url)) { return PREFIX + urls.long.get(url).id.toString(36) }
    const urlEntry = { url, id: urls.id };
    urls.long.set(url, urlEntry);
    urls.short.set(urlEntry.id, urlEntry);
    return PREFIX + urlEntry.id.toString(36);
}
function decode(url) {
    return urls.short.get(parseInt(url.split(PREFIX)[1], 36)).url;
}
</code></pre>
    </div>