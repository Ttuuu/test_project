<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<pre><code>import numpy as np
import math
def frosenbrock(x):

    x1 = x[0][0]
    x2 = x[1][0]
    val = 100. * (x2 - x1 ** 2.) ** 2. + (1 - x1) ** 2.
    return val;

def optimise_CMA(initial_guess):   
    #--------------INITIALIZATION----------------------
    N=2 #Number of variables, problem dimension
    #Container for coordinates called xmean
    xmean=initial_guess                           #Objective variables (coordinates)initial point
    if len(xmean) == 0:
        for i in range(int(N)):
            xmean.append([random.uniform(0,1)]);

    xmean=np.array(xmean,dtype=np.float_).reshape(N,1)
    sigma=0.5                              #Coordinate wise standard deviation
    stopfitness = 1e-10                    #stop if fitness &lt; stopfitness (minimization)
#     stopeval =1e3*N**2                     #stop after this number of evaluations ()
    stopeval = 100                    #stop after this number of evaluations ()


    #Strategy parameter setting: Selection
    lambd=int(4.0+math.floor(3.0*np.log(N)))                #population size, offspring number
    mu=int(lambd/2)                           #number of parents/points for recombination
    weights=np.array([])
    for j in range(mu):
        weights=np.append(weights,np.log(mu+0.5)-np.log(j+1))#muXone array for weighted recombination
    mu = math.floor(mu)
    weights=weights/sum(weights)
    mueff=sum(weights)**2/sum(weights**2)
    weights_array=weights.reshape([mu,1])  

    #Strategy parameter setting: Adaptation
    cc=(4+mueff/N)/(N+4 + 2*mueff/N)      #time constant for cumulation for C
    cs=(mueff+2)/(N+mueff+5)              #t-const for cumulation for sigma control 
    c1=2/((N+1.3)**2+mueff)               #learning rate for rank-one update of C
    cmu=min(1-c1,2*(mueff-2+1/mueff)/((N+2)**2+mueff)) #and for rank-mu update
    damps =1 +2*max(0,np.sqrt((mueff-1)/(N+1))-1)+cs  #damping for sigma, usually close to 1

    #Initialize dynamic (internal) strategy parameters and constants
    pc=np.zeros((N,1),dtype=np.float_)                      #evolution path for C  
    ps=np.zeros((N,1),dtype=np.float_)                      #evolution path for sigma 
    D=np.ones((N,1),dtype=np.float_)                        #diagonal D defines the scaling  
    B =np.eye(N,dtype=np.float_)                #B defines the coordinate system
    D_sqd=D**2           
    diag_D_sqd=(np.diag(D_sqd[:,0]))       #Generate diagonal matrix with D_sqd in the diagonal, the rest are zeros 
    trans_B=B.transpose()                 #Tranpose matrix of B 
    C=np.dot(np.dot(B,diag_D_sqd),trans_B)#Covariance matrix C
    nega_D=D**(-1)                        #Elementwise to power -1 
    diag_D_neg=(np.diag(nega_D[:,0]))     #Generate diagonal matrix with nega_D in diagonal, rest zeros
    invsqrtC =np.dot(np.dot(B,diag_D_neg),trans_B)#C^-1/2 
    eigeneval = 0                         #track update of B and D
    chiN=N**0.5*(1-1/(4*N)+1/(21*N**2))   #expectation of ||N(0,I)|| == norm(randn(N,1)) 

    #---------GENERATION LOOP---------------
    counteval=0
    arx=[]
    best_fitness = 100000000
    while counteval&lt;stopeval:
        #Generate and evaluate lamba offspring
        itera=1
        arfitness=[]
        for l in range(lambd):
            offspring=[]                  #Create a container for the offspring 
            offspring = xmean + sigma * np.dot(B,(np.multiply(D,np.random.standard_normal((N,1))))) #m + sig * Normal(0,C)
            if itera==1:
                arx=offspring
            else:
                arx=np.hstack((arx,offspring))
            arfitness.append(frosenbrock(offspring))          #EVALUATE OBJ FUNCTION
            counteval=counteval+1
            itera=itera+1   
        #Sort by fitness and compute weighted mean into xmean
        ordered=[]
        ordered=sorted(enumerate(arfitness), key=lambda x: x[1])    #minimization, list with (Index,Fitness) elements
        xold=xmean
        best_off_indexes=[]
        for i in range(int(mu)):
            best_off_indexes.append(ordered[i][0]) #List of best indexes of mu offspring
        recomb=np.zeros([N,mu])
        cont=0
        for index in best_off_indexes:
            recomb[:,cont]=arx[:,index]
            cont=cont+1
        xmean=np.dot(recomb,weights_array)

        #Cumulation: update evolution paths
        ps=(1-cs)*ps+np.sqrt(cs*(2-cs)*mueff)*np.dot(invsqrtC,(xmean-xold))*(1/sigma)
        hsig=np.linalg.norm(ps)/np.sqrt(1-(1-cs)**(2*counteval/lambd))/chiN &lt; 1.4 + 2/(N+1)
        if hsig==True:
            hsig=1
        else:
            hsig=0
        pc=(1-cc)*pc+hsig*np.sqrt(cc*(2-cc)*mueff)*(xmean-xold)/sigma

        #Adapt covariance matrix C
        artmp=(1/sigma)*(recomb-np.tile(xold,(1,mu)))
        weights_diag=np.diag(weights_array[:,0])
        C=(1-c1-cmu)*C+c1*(np.dot(pc,pc.transpose())+((1-hsig)*cc*(2-cc)*C))+cmu*np.dot(artmp,np.dot(weights_diag,artmp.transpose()))

        #Adapt step size sigma
        sigma=sigma*np.exp((cs/damps)*(np.linalg.norm(ps)/chiN - 1))

        #Decomposition of C into B*diag(D.^2)*B' (diagonalization)
        if counteval-eigeneval &gt; lambd/(c1+cmu)/N/10:
            eigeneval=counteval                    #to achieve O(N^2)
            C=np.triu(C)+np.triu(C,1).transpose()  #enforce symmetry
            B=np.linalg.eig(C)[1]                  #eigen decomposition, B==normalized eigenvectors
            diag_D=np.linalg.eig(C)[0]
            diag_D=diag_D.reshape([N,1])
            D=diag_D**0.5                       #D is a vector of standard deviations now
            D_inv=D**(-1)
            D_inv=np.diag(D_inv[:,0])
            invsqrtC=np.dot(B,np.dot(D_inv,B.transpose()))
#             print (ordered[0][1]);               # Uncomment to see convergence in the console
        #Break, if fitness is good enough or condition exceeds 1e14, better termination methods are advisable           
#         del arfitness
#         if ordered[0][1]&lt;=stopfitness or max(D)&gt;1e7*min(D):
#             print('bad convergence')
#             break
#         else:
#             del arx
#             del ordered
        #Return best point at last iteration


        if min(arfitness) &lt; best_fitness:
            best_fitness = min(arfitness)
            best_conf = arx[:,best_off_indexes[0]]
    xmin=arx[:,best_off_indexes[0]]
    print(best_fitness,best_conf)
    return xmin
</code></pre>

<p>xmean must be a 2d array and offspring formulation was wrong.
D .* randn(nGenes,1) involves Element wise multiplication. </p>

<pre><code>offspring = xmean + sigma * np.dot(B,(np.multiply(D,np.random.standard_normal((N,1))))) #m + sig * Normal(0,C)
</code></pre>
    </div>