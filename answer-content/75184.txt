<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Depending on how you are running your tests, here are a few things to consider:</p>

<ol>
<li>Startup time: Which portion of the code are you timing? Opening the file, getting the file size, etc, and other operations can take time. These operations will be take a relatively fixed amount of time, and won't slow down larger files.</li>
<li>Hard drive latency: <a href="http://en.wikipedia.org/wiki/Hard_disk_drive" rel="nofollow">According to Wikipedia</a>, the average seek latency is ~4ms. If the file isn't already cached by the OS, it will take (on average) that long to even begin to read.</li>
<li>Data transfer limits: According to the same article, the average HD transfer rate is ~1000Mbit/s. If actually reading the data is taking 1ms (see #2) that's a data transfer rate of 800MBits which doesn't leave a lot of room for improvement (assuming you're using a 7200rpm drive). If you're using a 5400rpm drive the data transfer rate can be expected to be lower.</li>
<li>Memory allocation/copying: Probably not going to be the main time sink, but could be optimized by not allocating memory for each file. If you have an upper limit to the size of the files, you can pre-allocate a buffer of that size, fill it in your function, and return the size. Depending on the rest of your architecture, you could get away with a single buffer, or you might have to use a pool.</li>
<li>Avoid the HD: If the files are being produced on the local machine, can they be written to shared memory or a ramdisk? That would significantly increase the seek times and data transfer rates.</li>
</ol>
    </div>