<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>You should probably start with a profiler whenever you have unexplained
performance problems, that should quickly point you to the functions or
areas that take most of the runtime.</p>

<p>That said, I immediately noticed the <code>json.load</code> and <code>json.dump</code> are
called more than once.  That seems to be at least a first candidate for
optimisation.  Either start keeping everything in RAM until you're ready
to write it to disk, or perhaps write it to disk while you're still
scraping (if you need any postprocessing: do it all at once when all
data has been collected).</p>

<p>The program also doesn't run without a previously set up JSON output
file, that's definitely worth fixing.</p>

<p>Yeah, so after a few seconds I've got like 100kb already.  If this runs
for 20 hours, the amount of time spent on parsing and dumping this data
over and over and over is gonna be quite a bit - and it's going get slower the further on this is running, so while at the start the impact measured might not be that much, it's just gonna increase.</p>

<p>There's no logging, so of course it's also hard to see what the program
is doing.  Consider putting in maybe the URLs, or a dot every 100 pages,
or whatever thing to make it easier to spot progress (or the lack
thereof).</p>

<p>Come to think of it, unless you've ruled out that possibility, the
scraping might get throttled by the website.</p>

<p>Edit: That reminds me, each page is fetched sequentially and there'll be a lot of waiting for network I/O.  The next obvious thing would be to do the fetching concurrently.  Have a couple of worker threads / processes and fetch multiple pages at the same time, getting new work items from a shared queue or so, then write results to disk in another thread.</p>

<hr>

<p>I'm gonna suggest using <code>cProfile</code> here, but just take a look at
<a href="https://docs.python.org/3/library/debug.html" rel="noreferrer">the reference</a> too.</p>

<p>In particular, try this:</p>

<pre><code>python3 -m cProfile -o profile laurence.py
</code></pre>

<p>Wait a moment, then abort the run.  Next, inspect the output:</p>

<pre><code>python3 -m pstats profile
</code></pre>

<p>Use <code>sort</code> and <code>stats</code>, like <code>sort time</code> and <code>stats 10</code> or so to get an
overview.</p>

<p>And it really looks like the I/O is the biggest reason from the very small
sample.</p>

<hr>

<p>Lastly, parsing via <code>lxml</code> into a full DOM tree might be slow too, plus
evaluation of the XPath queries (convenient as they might be).  You
could always explore a SAX or similar streaming parser.  In Python
that's for example
<a href="https://docs.python.org/3/library/html.parser.html" rel="noreferrer"><code>html.parser</code></a> -
not sure how it looks like from the compatibility perspective of course.</p>
    </div>