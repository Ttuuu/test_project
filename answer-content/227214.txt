<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I think in general <code>MinBy</code> and <code>MaxBy</code> are useful but handling all the <code>null</code> cases might be tricky, and indeed it is seeing the repeated logic and two similar loops. For this reason I also think that it'd be easier if we do not handle them at all but let the user decide what he wants to do with them and implement both simply with <code>OrderBy(Descending).First()</code></p>

<pre><code>public static TSource MinBy&lt;TSource, TKey&gt;
(
    this IEnumerable&lt;TSource&gt; source,
    Func&lt;TSource, TKey&gt; keySelector,
    IComparer&lt;TKey&gt; comparer = null
)
{
    return 
        source
            .OrderBy(s =&gt; keySelector(s), comparer ?? Comparer&lt;TKey&gt;.Default)
            //.OrderByDescending() // for MaxBy
            .First();
}
</code></pre>

<p>If the user wants <code>null</code>s to be sorted then he'd just call it with <code>.?</code></p>

<pre><code>sequence.MinBy(x =&gt; x?.Value);
</code></pre>

<p>and if not, then he can filter them out:</p>

<pre><code>sequence.Where(x =&gt; x != null).MinBy(x =&gt; x.Value);
</code></pre>

<p>Now, he has all the freedom and we no longer need to decide for him or document any possibe <em>tricks</em> or hidden features. If he doesn't filter <code>null</code>s out and doesn't use <code>.?</code> it'll just throw.</p>

<hr>

<p><strong>Don't use <code>SortedSet</code></strong> It turns out that it isn't the right tool for <strong>this</strong> job. <code>OrderBy[Descending]</code> however, is performing quite well (even much better than expected) and only marginally left behind by the single-pass approach. </p>

<p><sup>(Tested with extendend benchmarks with 100k shuffled items where the filter value was a random number between 0-100)</sup></p>

<hr>

<p><sup>obsolete</sup></p>

<p><strike>In performance critical scenarios I'd use a <code>SortedSet</code> with a custom wrapper-comparer (also because I'm lazy). Its complexitiy is <strong>O(log n)</strong> according to <a href="https://www.c-sharpcorner.com/UploadFile/0f68f2/comparative-analysis-of-list-hashset-and-sortedset/" rel="nofollow noreferrer">Comparative Analysis of List, HashSet and SortedSet</a> which is better than <strong>O(n)</strong> according to <a href="https://stackoverflow.com/a/2307314/235671">What does O(log n) mean exactly?
</a>. The disadvantage might be that the set needs aditional memory to hold the items but maybe this can be neglected and the faster search outweighs the memory usage.</strike></p>

<pre><code>private class MinComparer&lt;TSource, TKey&gt; : IComparer&lt;TSource&gt;
{
    public IComparer&lt;TKey&gt; Comparer { get; set; }
    public Func&lt;TSource, TKey&gt; KeySelector { get; set; }
    public int Compare(TSource x, TSource y)
    {
        return Comparer.Compare(KeySelector(x), KeySelector(y))
    }
}

public static TSource MinBy&lt;TSource, TKey&gt;
(
    this IEnumerable&lt;TSource&gt; source,
    Func&lt;TSource, TKey&gt; keySelector,
    IComparer&lt;TKey&gt; comparer = null
)
{
    var c = new MinComparer&lt;TSource, TKey&gt;
    {
        KeySelector = keySelector,
        Comparer = comparer ?? Comparer&lt;TKey&gt;.Default,
    };

    return new SortedSet&lt;TSource&gt;(source, c).First();               
}
</code></pre>

<p>Another <code>MaxComparer</code> could order the items in descending order so that we still could use <code>First</code> with it.</p>
    </div>