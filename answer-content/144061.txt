<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>The obligatory other solution solving your life:</p>

<pre><code>def timer(f):
    def wrapper(job_args, *args, **kwargs):
        fn_args, timeout, timeout_callback = job_args[:3]
        q = Queue()
        p = Process(target=f, args=(q, fn_args), kwargs=kwargs)
        p.start()
        p.join(timeout=timeout)
        p.terminate()
        p.join()
        if not q.empty():
            return q.get()
        return timeout_callback(fn_args, args, kwargs)
    return wrapper


@timer
def job(q, file, *args, **kwargs):
    sleep(3)
    print(file, getpid())
    q.put(file+"_done")


def timeout_callback(*args, **kwargs):
    print("Timeout")


def main():
    timeout = 2
    data = ["file1", "file2", "file3", "file4", "file5"]
    tp = ThreadPoolExecutor(2)

    data = [(x, timeout, timeout_callback) for x in data]
    for got in tp.map(job, data):
       print(got)
</code></pre>

<hr>

<p>In your <strong>init</strong></p>

<pre><code>def __init__(self, max_workers: int = None):
        ...
        super().__init__()
</code></pre>

<p>but you don't have a super class in</p>

<pre><code>class ProcessKillingExecutor:
</code></pre>

<p>you could add </p>

<pre><code>class ProcessKillingExecutor(object):
</code></pre>

<p>for clarity, else it looks as if you're calling a the super of a base class. </p>

<hr>

<p>The manager might be unnecessary; you are only ever transferring one value. What you are looking for might be a Queue. </p>

<hr>

<p>You are only allowed 80 characters, because it becomes more readable</p>

<pre><code>"""
    The ProcessKillingExecutor works like an `Executor &lt;https://docs.python.org/dev/library/concurrent.futures.html#executor-objects&gt;`_
    in that it uses a bunch of processes to execute calls to a function with different arguments asynchronously.

    But other than the `ProcessPoolExecutor &lt;https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor&gt;`_,
    the ProcessKillingExecutor forks a new Process for each function call that terminates after the function returns or
    if a timeout occurs.

    This means that contrary to the Executors and similar classes provided by the Python Standard Library, you can
    rely on the fact that a process will get killed if a timeout occurs and that absolutely no side can occur between
    function calls.

    Note that descendant processes of each process will not be terminated – they will simply become orphaned.
    """
</code></pre>

<p>do it like this. </p>

<pre><code>"""
    The ProcessKillingExecutor works like an `Executor
    &lt;https://docs.python.org/dev/library/concurrent.futures.html#executor-objects&gt;`_
    in that it uses a bunch of processes to execute calls to a function with
    different arguments asynchronously.

    But other than the `ProcessPoolExecutor
    &lt;https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor&gt;`_,
    the ProcessKillingExecutor forks a new Process for each function call that
    terminates after the function returns or if a timeout occurs.

    This means that contrary to the Executors and similar classes provided by
    the Python Standard Library, you can rely on the fact that a process will
    get killed if a timeout occurs and that absolutely no side can occur
    between function calls.

    Note that descendant processes of each process will not be terminated –
    they will simply become orphaned.
    """
</code></pre>

<p>The exemptions being urls, or in other words, it's good pratice to not line break urls. </p>

<hr>

<p>Your <strong>Annotations</strong> is slightly off  </p>

<pre><code>def submit(self, func: Callable = None, args: Any = (), kwargs: Dict = {}, timeout: float = None,
               callback_timeout: Callable[[Any], Any] = None, daemon: bool = True):
</code></pre>

<p>this should be like this with no whitespace and considering that the parameters will be there:</p>

<pre><code>def submit(self,
           func: Callable,
           fn_args: Any,
           p_kwargs: Dict,
           timeout: float,
           callback_timeout: Callable[[Any], Any],
           daemon: bool):
</code></pre>

<hr>

<p><strong>Encapsulation</strong>: You are not working with <strong>kwargs and args</strong>, so don't name them as such, in the same sense that you should not name your variable <code>i</code>; it is very confusing.  </p>

<p>You are dealing with three different args and kwargs, namely the job, the executing process and the classes, and args and kwargs in the class, should belong to the class if needed or not. </p>

<pre><code>params = ({'func': func, 'fn_args': p_args, "p_kwargs": {},
                   'timeout': timeout, 'callback_timeout': callback_timeout,
                   'daemon': daemon} for p_args in iterable)
</code></pre>

<hr>

<p><strong>Terminating and joining</strong> the processes of a program.</p>

<pre><code>if p.is_alive():
        p.terminate()
        p.join()
</code></pre>

<p>Terminating and joining is not the same thing. Check out your task-manager, you should see the processes staying as "zombies". The call <code>terminate()</code> forces the exit of the process and <code>join()</code> does something I don't pretend to understand, but I know that if you don't call join on terminated processes, you will get zombies. </p>

<hr>

<p>It's confusing that you use a <strong>manager</strong>; I get the impression that you are going to use the result to handle interchange of data between but processes, this is not the case. You are just retrieving a result from from a child process. </p>

<p>It is dangerous because you are always overwriting the same key "result" in the manager dict. Think of it. Every time you return from the manager.dict, there can be leakage - i.e. if it is overwritten before you return, the wrong result is returned. </p>

<p>You should use a Queue that is unique to every </p>

<pre><code>p.start()
p.join(timeout=timeout)
if not queue.empty():
    return queue.get()
if callback_timeout:
    callback_timeout(*p_args, **p_kwargs)
if p.is_alive():
    p.terminate()
    p.join()
</code></pre>

<p>and </p>

<pre><code>@staticmethod
def _process_run(queue: Queue, func: Callable[[Any], Any]=None, 
                 *args, **kwargs):
    """
    Executes the specified function as func(*args, **kwargs).
    The result will be stored in the shared dictionary
    :param func: the function to execute
    :param queue: a Queue
    """
    queue.put(func(*args, **kwargs))
</code></pre>

<hr>

<p>This</p>

<pre><code>    return None
</code></pre>

<p>is also unnecessary because, <code>None</code> is always returned from a python function that does not return anything. </p>

<hr>

<p>Your imports</p>

<pre><code>import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Manager, Process
from typing import Callable, Iterable, Dict, Any
</code></pre>

<p>should be</p>

<pre><code>import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Process
from multiprocessing import Queue
from typing import Callable
from typing import Iterable
from typing import Dict
from typing import Any
</code></pre>

<p>as per convention. </p>

<hr>

<p>I ended up with this, and I don't claim that it is any better then yours. But I'll show it anyway for completeness.  </p>

<pre><code>import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Process
from multiprocessing import Queue
from typing import Callable
from typing import Iterable
from typing import Dict
from typing import Any


class ProcessKillingExecutor:
    """
    The ProcessKillingExecutor works like an `Executor
    &lt;https://docs.python.org/dev/library/concurrent.futures.html#executor-objects&gt;`_
    in that it uses a bunch of processes to execute calls to a function with
    different arguments asynchronously.

    But other than the `ProcessPoolExecutor
    &lt;https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor&gt;`_,
    the ProcessKillingExecutor forks a new Process for each function call that
    terminates after the function returns or if a timeout occurs.

    This means that contrary to the Executors and similar classes provided by
    the Python Standard Library, you can rely on the fact that a process will
    get killed if a timeout occurs and that absolutely no side can occur
    between function calls.

    Note that descendant processes of each process will not be terminated –
    they will simply become orphaned.
    """

    def __init__(self, max_workers: int=None):
        self.processes = max_workers or os.cpu_count()

    def map(self,
            func: Callable,
            iterable: Iterable,
            timeout: float=None,
            callback_timeout: Callable=None,
            daemon: bool = True
            ) -&gt; Iterable:
        """
        :param func: the function to execute
        :param iterable: an iterable of function arguments
        :param timeout: after this time, the process executing the function
                will be killed if it did not finish
        :param callback_timeout: this function will be called, if the task
                times out. It gets the same arguments as the original function
        :param daemon: define the child process as daemon
        """
        executor = ThreadPoolExecutor(max_workers=self.processes)
        params = ({'func': func, 'fn_args': p_args, "p_kwargs": {},
                   'timeout': timeout, 'callback_timeout': callback_timeout,
                   'daemon': daemon} for p_args in iterable)
        return executor.map(self._submit_unpack_kwargs, params)

    def _submit_unpack_kwargs(self, params):
        """ unpack the kwargs and call submit """

        return self.submit(**params)

    def submit(self,
               func: Callable,
               fn_args: Any,
               p_kwargs: Dict,
               timeout: float,
               callback_timeout: Callable[[Any], Any],
               daemon: bool):
        """
        Submits a callable to be executed with the given arguments.
        Schedules the callable to be executed as func(*args, **kwargs) in a new
         process.
        :param func: the function to execute
        :param fn_args: the arguments to pass to the function. Can be one argument
                or a tuple of multiple args.
        :param p_kwargs: the kwargs to pass to the function
        :param timeout: after this time, the process executing the function
                will be killed if it did not finish
        :param callback_timeout: this function will be called with the same
                arguments, if the task times out.
        :param daemon: run the child process as daemon
        :return: the result of the function, or None if the process failed or
                timed out
        """
        p_args = fn_args if isinstance(fn_args, tuple) else (fn_args,)
        queue = Queue()
        p = Process(target=self._process_run,
                    args=(queue, func, fn_args,), kwargs=p_kwargs)

        if daemon:
            p.deamon = True

        p.start()
        p.join(timeout=timeout)
        if not queue.empty():
            return queue.get()
        if callback_timeout:
            callback_timeout(*p_args, **p_kwargs)
        if p.is_alive():
            p.terminate()
            p.join()

    @staticmethod
    def _process_run(queue: Queue, func: Callable[[Any], Any]=None,
                     *args, **kwargs):
        """
        Executes the specified function as func(*args, **kwargs).
        The result will be stored in the shared dictionary
        :param func: the function to execute
        :param queue: a Queue
        """
        queue.put(func(*args, **kwargs))


def some_task(n, *args, **kwargs):
    import time
    time.sleep(n/4)
    return n ** 2


if __name__ == "__main__":
    def fun_timeout(n):
        print('timeout:', n)
    executor = ProcessKillingExecutor(max_workers=2)
    generator = executor.map(some_task, [1, 1, 2, 2, 3, 3, 4, 4], timeout=2,
                             callback_timeout=fun_timeout)
    for elem in generator:
        print(elem)
</code></pre>

<hr>

<p>What would be the coolest solution? It would be to have a class inherit from <code>ThreadPoolExecutor</code> and override the specific part of the class that executes the the threads inherent to <code>ThreadPoolExecutor</code> with what you want to do.</p>
    </div>