<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>If you would define a model like this:</p>
<pre><code>public class Data
{
  public string Id { get; set; }
  public string OCR { get; set; }
  public string Rule {get; set; }
  public string Output {get; set; }
}
</code></pre>
<p>then you could easily separate your <strong>ETL</strong> job's different stages.</p>
<p>For example the <em>Extract</em> phase would look like this:</p>
<pre><code>Document doc = XDocument.Parse(xml);
var parsedData = from data in doc.Descendants("Data")
                 select new Data()
                 {
                      Id = (string)data.Attribute("id"),
                      OCR = (string)data.Element("ocrstring"),
                      Rule = (string)data.Element("rule")
                 };
</code></pre>
<p>In your <em>Transform</em> phase you could perform the regex based transformations. The biggest gain here is that it is free from any input or output format. It is just pure business logic.</p>
<p>And finally in your <em>Load</em> phase you could simply serialize the whole (modified) data collection. Or if it is too large, then create logic to find the appropriate element (based on the <code>Id</code> property) and overwrite only the <code>output</code> child element.</p>
<hr>
<p>What you have gained here is a pretty nice separation of concerns.</p>
<ul>
<li>Your read logic is not mixed with the processing logic.</li>
<li>Because of the separation it is easier to spot where is the bottleneck of
the application (if any).</li>
<li>Input format can be changed without   affecting processing logic.</li>
<li>Pipeline like processing can be introduced to improve performance by invoking processing right after a <code>Data</code> object has been populated from the source.</li>
<li>Many other advantages. :)</li>
</ul>
    </div>