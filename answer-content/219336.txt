<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>There's no reason to use selenium here. Instead of reaching for selenium, first try the easier path. Load the page without javascript and see if you can find any helpful information about how the images get there. There must be something (perhaps AJAX) that gets the URLs for them. They don't just magically appear!</p>

<p>It turns out if you do this, you'll see there isn't any fancy JS stuff. The images are there, they just look like this:</p>

<pre class="lang-html prettyprint-override"><code>&lt;div class="image"&gt;
  &lt;img class="lazyload" data-src="/image/th/QD-H-4F3JpxykaxnIIFbrixlkt4rwBphjoSmX2E8fvoJ8JanT-+S2MdyjKTADNm+SJdCVhXQwkdIZ0tQel-n8-y70M9EOTmeW06uA5ubLwnl2gi5X14+yw6GKhNbhj7S.jpg"&gt;
&lt;/div&gt;
</code></pre>

<p>This means, you can extract all these URLs with a single line of BeautifulSoup:</p>

<pre class="lang-py prettyprint-override"><code>urls = [img['data-src'] for img in doc.find_all('img', class_='lazyload')]
</code></pre>

<p>Now some comments about your code:</p>

<ul>
<li>PEP8 it! Your spacing is inconsistent. Make good use of vertical whitespace. Phrase things like you would paragraphs. That makes things much easier to read.</li>
<li>You don't need selenium, but you definitely shouldn't hard code a driver path in a project that you open source. How many people have a selenium driver exactly at <code>C:\Users\NH\PycharmProjects\SeleniumTest\drivers\chromedriver.exe</code> on their computer?</li>
<li>Nice use of functions to separate concerns</li>
<li>You should probably use <code>BeautifulSoup(page, 'html5lib')</code> instead of lxml</li>
<li>Your construction of <code>image_url</code> is a bit sloppy. Typically, we'd reach for <code>urllib.path</code> to build paths instead of just doing string concatenation.</li>
<li>Use <code>pathlib</code> instead of <code>os.path</code></li>
<li><code>'Mozilla/5.0'</code> isn't a User-Agent that's going to fool anybody. If you're really trying to stay under the radar, use a real UA</li>
<li>But none of that matters, because you appear to request pages as fast as possible. Add <code>sleep()</code>s in between downloading. Throttle your scraper.</li>
<li><code>threading</code> is a bit useless in Python. This is somewhat of an I/O bound task (for which threads are well suited), but the HTML parsing and extraction could definitely be done concurrently with web requests (but threading doesn't allow this). You almost always want to reach for <code>multiprocessing</code>.</li>
<li>Use the context manager of a pool instead of manually calling <code>close()</code> and <code>join()</code>:</li>
</ul>



<pre class="lang-py prettyprint-override"><code>with Pool() as pool:
    pool.imap_unorderd(fetch_image_url, pages)
</code></pre>

<ul>
<li>Also, don't pass a parameter to <code>Pool</code>. It defaults to the number of CPU cores, which is almost always what you want</li>
<li><code>starmap</code> is ordered and blocking. It only can process things in order. This is okay in this case, because you aren't actually returning anything, but if you were say doing math you probably want <code>imap_unordered</code> which yields results as they arrive (likely out of order).</li>
<li>Don't <code>print</code> from a separate process. You want a single process writing to stdout, otherwise you can have write contention (which you'll luck out and probably never run into because your strings probably fit inside the stdout buffer, but it's possible they may not under certain circumstances).</li>
</ul>

<p>But this all culminates in the following advice: don't use Python to download things!</p>

<p>Especially since this scraping doesn't appear to be useful as a library (instead, it seems like you just are providing a CLI utility for a human to download these things). Given this, it's much smarter and safer to not reinvent the wheel. There are tools that already do jobs like this well: namely, <code>wget</code> (it appears that you're on windows, you can and should use the Ubuntu subsystem, which will have <code>wget</code>). <code>wget</code> is particularly suited for this job and has <em>tons</em> of builtin functionality which will be super useful to you. This includes:</p>

<ul>
<li>Not redownloading things</li>
<li>Throttling (including random delays)</li>
<li>Restarting after a catastrophic (program crashing) failure</li>
<li>Retrying requests per HTTP spec</li>
</ul>

<p>All of these are things that your script doesn't do currently. In particular, in python it's very easy to do something like this:</p>

<pre class="lang-py prettyprint-override"><code>pages = download_hundreds_of_pages()  # takes hours...
for page in paages:  # oops, this NameErrors and you lose everything you've downloaded
  pass
</code></pre>

<p>Mistakes like this are too easy to make. You can completely avoid them with the following workflow:</p>

<ol>
<li>Build up a list of urls you want to download (perhaps with python)</li>
<li>Use <code>wget -nc -i urls.txt</code> to download them</li>
<li>Repeat as necessary</li>
</ol>

<p>For you, that would involve making a list of urls containing the images. Then do <code>wget -nc -i pages.txt</code>. That will download all of the pages to the current directory. Then you can make a Python script which uses beautiful soup (and the line I mentioned above) to extract the image urls: <code>python3 extract_image_urls.py &gt; image_urls.txt</code>. Then to download them do <code>wget -nc -i image_urls.txt</code>. If your python script fails at any point, you don't lose all of the downloads you've already done. You can wrap all of this in a convenient bash script.</p>
    </div>