<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>To partially answer question (6), the Rehash function can be modified to start:</p>

<pre><code>private void Rehash( byte[] input, int position, uint hash, int length )
{
  while ( true )
  {
    if ( length &gt; MaxMatch ) return;
</code></pre>

<p>The logic is that since the limit on a match is MaxMatch (=258), there is no point hashing a string longer than that. This means it is now possible, albeit rather slow, to compress long repeat sequences ( which are quite common when compressing PDF images ). </p>

<p>I'm not very satisfied with this solution yet though, I think the algorithm could be modified to process long repeats efficiently and elegantly, but I haven't figured out how. Even with this fix, long repeats cause the processing to go much slower than normal - the outer loop of Rehash is executed nearly 258 times for each byte of input processed, when processing a long repeat.</p>

<p>Edit: I think skipping to the last MaxMatch bytes of a repeat sequence might work, as earlier potential matches will never be used ( again due to the MaxMatch limit ). I have not yet tried to code this.</p>

<p>Further Edit: After more investigation, my conclusion is that the re-hashing is not an effective approach. The cost appears to exceed any benefit, at least in the context of RFC 1951. </p>

<p>What seems to be more interesting is trying different block sizes, which seems to produce improved compression for relatively mimimal cost ( for example, for each block, starting with a small block size, and testing whether doubling the block size results in smaller output ).</p>
    </div>