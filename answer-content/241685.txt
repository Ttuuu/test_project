<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h2>Encoding polynomials</h2>

<p>According to your code, you represent a polynomial 
<span class="math-container">$$\sum\limits_{k=0}^{n} a_kx^k$$</span> as <code>[a_1, ..., a_n, a_0]</code> which is odd to my eyes.</p>

<p>The most common way to represent a polynomial is probably<code>[a_n, ..., a_1, a_0]</code>. Then for example your <code>predict</code> function becomes</p>

<pre><code>def predict(self, x: float):
    return np.vander([x], len(self.weights)).dot(self.weights)
</code></pre>

<p>which is vectorised (by using <code>.dot</code>), so it should be a bit faster.
On the other hand, we can vectorise it further by allowing vectorial inputs:</p>

<pre><code>def predict(self, x):
    return np.vander(x, len(self.weights)).dot(self.weights)
</code></pre>

<p>This allows us to evaluate things like <code>predict(np.array([-1, 0, 1]))</code>.</p>

<p>One consequence is that in your error calculation code you can write something like</p>

<pre><code>mean_sq_error = ((predict(X) - y)**2).mean()
</code></pre>

<p>which is vectorised and easy to read.</p>

<h2>Calculating the gradients</h2>

<p>In (the euclidean norm) <code>norm_2</code> polynomial fitting reduces to finding <code>weights</code> such that the value of </p>

<pre><code>norm_2(vander(x).dot(weights) - y)
</code></pre>

<p>is minimal. The minimum point doesn't change if we compose <code>norm_2</code> by some non-decreasing function from the left, so e.g. using any of</p>

<pre><code>norm_2_sq = (^ 2)   . norm_2
mse_norm  = (* 1/n) . (^ 2) . norm_2
rmse_norm = (^ 1/2) . mse_norm
</code></pre>

<p>will result in the same minimum points. The most natural of these is arguably <code>norm_2_sq</code>.</p>

<p>Let us generalize using this norm. Given a matrix <code>A</code>, and a vector <code>b</code>, we'd like to find <span class="math-container">$$\operatorname{argmin}_x \| Ax - b \|_2^2,$$</span> 
but <span class="math-container">$$\| Ax - b \|_2^2 = (Ax -b)^T (Ax-b) = x^TA^TAx - x^TA^Tb -b^TAx-b^Tb,$$</span>
so its gradient is
<span class="math-container">$$
2x^T A^T A - 2b^TA.
$$</span>
If you want, you can use this to calculate the gradients of <code>mse</code>, <code>rmse</code> using the chain rule. You don't need to use <code>approx_fprime</code>.</p>

<p>On the other hand, since its second derivative, <code>2A'A &gt;= 0</code>, our functional is convex, so it takes its global minimum at the zero of its gradient, i.e. the solution of the so-called normal equation:
<span class="math-container">$$
x^T A^TA = b^TA \quad \Leftrightarrow \quad A^TA x = A^T b.
$$</span></p>

<p>As a practice problem, you can solve this equation using some iterative method (e.g. the conjugate gradient method).</p>

<h2>General comments</h2>

<p>The general consensus is that function names should be written in <code>snake_case</code>, class names in <code>CamelCase</code></p>

<p>There are a few unnecessary spaces <code>RandomState( 1)</code>, parentheses <code>x = (np.linspace(1,5,100))</code>. <code>class PolynomialRegression:</code> is sufficient (no <code>()</code> needed).</p>

<p>Given the ML context, I would reserve <code>weights</code>, <code>bias</code> for denoting the weights of a(n affine-) linear mapping. </p>

<p>Despite writing</p>

<pre><code>    if loss == 'MSE':
        loss = MSE
        self.loss_type = 'MSE'
    elif loss == 'RMSE':
        loss = RMSE
        self.loss_type = 'RMSE'
</code></pre>

<p>you hard-coded <code>MSE</code> a few lines later.</p>

<p>Tangentially related: <code>globals()[loss]</code> would be the method named by the value of <code>loss</code>, assuming this method is defined globally.</p>
    </div>