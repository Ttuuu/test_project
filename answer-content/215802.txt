<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Your code is fairly readable, but I find it quite dense. Here are some general things you could do to improve it:</p>

<ol>
<li><p>Use vertical white space to separate different areas of the code.</p></li>
<li><p>Provide a docblock at the top outlining the algorithm you are using.</p></li>
<li><p>Use more than one function to break your code into smaller chunks.</p></li>
</ol>

<p>With that advice out of the way, let's look at your performance with an eye to the worst possible case:</p>

<pre><code>def textQueries(sentences, queries):
    for query in queries:             # Worst case: 10,000 times
        ...
        for query_word in query_words:  # Worst case: 10 times
            if query_word in word_map:
                    final_set.intersection  # Worst case: O(10,000) in C
            else:
                for ... in enumerate(sentences):  # Worst case: 10,000 times
                if ... else:
                    final_set.intersection  # Worst case: O(10,000) in C
</code></pre>

<p>According to me, your code runs worst case in <span class="math-container">\$O(Q \times W \times S)\$</span> where <span class="math-container">\$Q\$</span> is the number of queries, <span class="math-container">\$W\$</span> is the number of words in the query, and <span class="math-container">\$S\$</span> is the number of sentences. (The number of words in the sentences is hidden behind a split, but it's in C so we'll let it go by.)</p>

<p>There's no question you need to split the sentences and split the queries into separate words, somehow. But you are <em>nesting</em> your loops when the "best" way to loop is to do them one after the other. If you can convert from this:</p>

<pre><code>for q in Q:
    for s in Q:
        pass
</code></pre>

<p>to something like this:</p>

<pre><code>for s in S:
    pass

for q in Q:
    pass
</code></pre>

<p>You will have converted from <span class="math-container">\$O(S \times Q)\$</span> to <span class="math-container">\$O(S + Q)\$</span> which is considerably faster.</p>

<p>Your current approach is to define a set of sentences that contain a particular word. You cache these sets, in case the same words appear again. You intersect the sets for each word in the query, producing a final set of all the sentences that contain all the query words.</p>

<p>This is pretty much <strong>guaranteed to fail</strong> based on the instructions you quoted:</p>

<blockquote>
  <p>No word appears in more than 10 sentences.</p>
</blockquote>

<p>In other words, the probability of a "cache hit" is 10/10,000, or 0.1%. Any time you spend maintaining the cache is wasted time.</p>

<p>Suppose you just go through the sentences one time, and build the big dictionary of sets:</p>

<pre><code>sentence_words = collections.defaultdict(set)
for i, s in enumerate(sentences):
    for w in s.split():
        sentence_words[s].add(i)
</code></pre>

<p>Then you can go through the query words and do what you were doing before, without any caching:</p>

<pre><code>for q in queries:
    matches = functools.reduce(operator.and_, (sentence_words[w] for w in q.split()))
    matches = [-1] if not matches else sorted(matches)
</code></pre>

<p>This will seem very similar to what you were doing, but the difference is that these two loops are run one after the other, providing something like <span class="math-container">\$O(100,000 + 100,000)\$</span> performance instead of <span class="math-container">\$O(100,000 \times 10,000)\$</span>. It's that last operator that makes all the difference.</p>
    </div>