<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<ol>
<li>Don't parse HTML manually, you should use the <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="nofollow noreferrer"><code>BeautifulSoup</code></a> module!</li>
<li><code>import</code> should be at the top of the file</li>
<li>Use a <code>if __name__ == '__main__'</code> guard</li>
<li>Functions and variable should be <code>snake_case</code></li>
</ol>

<hr>

<p>First you can rewrite the <code>getTable()</code> alot using the <code>BeautifulSoup</code> module</p>

<pre><code>import requests
from bs4 import BeautifulSoup

url = "http://webscraper.io/test-sites/tables"
soup = BeautifulSoup(requests.get(url).text, 'html.parser')
for table in soup.select('.table'):
    new_table = [[c.text for c in row.find_all('td')] for row in table.find_all('tr')]
</code></pre>

<p>The only problem is that it will also give back None values in the table, so we'd need to catch the None values and only yield when the list is not filled with None</p>

<h1>Revised Code</h1>

<pre><code>import requests
import pandas as pd
from bs4 import BeautifulSoup

def parse_table(table):
    for row in table.find_all('tr'):
        col = [c.text for c in row.find_all('td')]
        if not all(c is None for c in col):
            yield col

def scrape_tables():
    url = "http://webscraper.io/test-sites/tables"
    soup = BeautifulSoup(requests.get(url).text, 'html.parser')
    for table in soup.select('.table'):
        parsed_table = [col for col in parse_table(table)]
        df = pd.DataFrame(parsed_table, columns=["#", "First Name", "Last Name", "User Name"])
        print()
        print(df)

if __name__ == '__main__':
    scrape_tables()
</code></pre>
    </div>