<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<ul>
<li><p>The lookup table <code>lut</code> should be hard-coded in the source code and defined outside the kernel function as a <code>__constant</code> space global variable. As it is now, every thread would have to recalculate the entire table. Also they take too much space for thread's private memory space. Alternately, maybe pre-calculate it on the host and pass it to the kernel in a <code>__constant</code> space argument.</p></li>
<li><p>The outer <code>for</code> loop should be reformulated so that every thread has the same values for the counter variables, and the number of iterations is a compile-time constant. Then applying <code>#pragma unroll</code> may accelerate it.</p></li>
<li><p>When code diverges (on NVIDIA), it can become as slow as if the diverging section (i.e. the inner <code>do</code> loop) on threads within each warp (= 32 adjacent threads) were executed sequentially. Maybe somehow change it so that there is no divergence inside warps.</p></li>
</ul>

<p><strong>Edit:</strong></p>

<ul>
<li><p>Maybe hardcoding the LUT and putting it into <code>__global</code> memory instead. The GPU would put it into global memory anyways because it is too large for the per-thread private memory. And like that the values don't have to be calculated inside the kernel.</p></li>
<li><p>It that does not accelerate it, maybe make a copy of it in <code>__local</code> memory (per work group). And copy it from global to local inside the kernel. Then use the shared <code>lut</code>.</p></li>
<li><p>Do this copy in a <em>coalesced</em> manner, using multiple work items. For example on NVIDIA, each n'th work item of 32 must access the n'th item of 32 from the global memory table.</p></li>
<li><p>Take the first (always-executed) iteration of the inner <code>do</code> loop outside the loop, and make it execute the next iterations (in a loop), only if it is needed for at least one work item the subgroup, using the <a href="https://www.khronos.org/registry/OpenCL/sdk/2.0/docs/man/xhtml/cl_khr_subgroups.html" rel="nofollow noreferrer">subgroup functions</a>.</p></li>
<li><p>Use an OpenCL profiler (one exists for AMD) to see where the performance losses are. Or port it to CUDA, and use the NVidia Visual Profiler (from the CUDA SDK).</p></li>
<li><p>Pre-calculate if 128 bit integers are needed for each work item, or if 64 bit (or 32 bit) are sufficient. Then only use 128 bit integers when any work item in the subgroup needs it.</p></li>
</ul>
    </div>