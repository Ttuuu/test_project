<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Just one small contribution from me: I think your utilization of BeautifulSoup is not optimal. For example this bit of code is wasteful as it does not warrant using the <code>map</code> function:</p>

<pre><code>self.titles = list(
    map(BeautifulSoup.get_text, self.soup.find_all('div', class_='comic-title')))
</code></pre>

<p>What does the map function do ? From the <a href="https://docs.python.org/3/library/functions.html#map" rel="nofollow noreferrer">documentation</a> (emphasis is mine):</p>

<blockquote>
  <p>map(function, iterable, ...)</p>
  
  <p>Return an iterator that <strong>applies function to every item of iterable</strong>, yielding the results. If additional iterable arguments are
  passed, function must take that many arguments and is applied to the
  items from all iterables in parallel. With multiple iterables, the
  iterator stops when the shortest iterable is exhausted...</p>
</blockquote>

<p>A more straightforward of getting the same result (and trimming text) would be:</p>

<pre><code>self.titles = [title.get_text().strip() for title in self.soup.find_all('div', class_='comic-title')]
</code></pre>

<p>Or:</p>

<pre><code>self.titles = [title.get_text(strip=True) for title in self.soup.find_all('div', class_='comic-title')]
</code></pre>

<p>And there is no need to involve <code>BeautifulSoup.get_text</code> either. You've already loaded the soup, once is enough.</p>

<p>Another thing:</p>

<pre><code>self.comicinfo = [x.replace(u'\xa0', u'').strip()
                  for x in list(map(BeautifulSoup.get_text, self.soup.find_all('div', class_='comic-details comic-release')))
                  ]
</code></pre>

<p>Here you are trying to get rid of the <a href="https://en.wikipedia.org/wiki/Non-breaking_space" rel="nofollow noreferrer">non-breaking space</a> <br>
Although we are dealing we just one pesky character you might encounter more unwanted 'characters' in the future when scraping UTF-8 encoded pages.</p>

<p>Based on several posts like this <a href="https://stackoverflow.com/a/58692697/6843158">one</a> and <a href="https://towardsdatascience.com/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c" rel="nofollow noreferrer">this one</a> a possible strategy is to use the <a href="https://docs.python.org/3.5/library/unicodedata.html#unicodedata.normalize" rel="nofollow noreferrer"><code>unicodedata.normalize</code></a> function to derive canonical representations of those strings. Since the closest representation of a non-breaking space is of course a plain space, then we want a plain space.</p>

<p>In short this will give a cleaned-up string that is more usable:</p>

<pre><code>unicodedata.normalize("NFKD", 'Archie Comics路\xa0 Apr 8th, 2020 \xa0路\xa0 $7.99')

# output: 'Archie Comics路  Apr 8th, 2020  路  $7.99'
</code></pre>

<p>(and there using the map function makes sense I think)</p>

<p>The cost is importing one more dependency: <code>import unicodedata</code>
Admittedly that is not so easy to grasp and even experienced developers are having headaches with processing of Unicode text and character set conversions. But you can't really avoid those issues when doing scraping jobs, they will always torment you.</p>

<p>One more reference on the topic: <a href="https://en.wikipedia.org/wiki/Unicode_equivalence" rel="nofollow noreferrer">Unicode equivalence</a></p>
    </div>