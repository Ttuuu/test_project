<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>A few comments, unfortunately not on the multiprocessing part.</p>

<ul>
<li><p><code>parser.add_argument("--output", action='store_true', default=False)</code> is exactly the same as <code>parser.add_argument("--output", action='store_true')</code>, the <code>'store_true'</code> action makes sure that it is false if the flag is not set.</p></li>
<li><p>I like to give my argument parsing functions an optional argument, so <code>def parse_args(args=None)</code> and later use <code>return parser.parse_args(args)</code>. This allows you to interactively test this function by passing a list of strings to see if the parsing works as expected. When it is <code>None</code>, the parsing proceeds as it currently does.</p></li>
<li><p>Python 3 has <a href="https://www.python.org/dev/peps/pep-3132/" rel="nofollow noreferrer">advanced tuple unpacking</a>, so you could do <code>_, *features = line.split()</code> instead of <code>features = line.split()[1:]</code>. Whether or not that is better is debatable, but it is good to know that this feature exists.</p></li>
<li><p>While "indexes" is a valid plural of "index", if it is used in the mathematical sense, <a href="https://ell.stackexchange.com/questions/59/is-indices-or-indexes-the-plural-of-index">you should probably use "indices"</a>.</p></li>
<li><p><code>Counter</code> objects have a nice <a href="https://docs.python.org/3/library/collections.html#collections.Counter.update" rel="nofollow noreferrer"><code>update</code></a> method. It can either take another <code>Counter</code> (or actually any <code>dict</code> subclass) object, in which case it works just like the normal <code>dict.update</code>. But it can also take an iterable, in which case it consumes that iterable just like it does when creating the object (by counting the occurrences of each object). So </p>

<pre><code>indexes = line_process_fun(line)
for index in indexes:
    counter[index] += 1
</code></pre>

<p>Could just be</p>

<pre><code>counter.update(line_process_fun(line))
</code></pre></li>
<li><p>Indeed, that whole function could be greatly simplified by using <code>map</code> and <a href="https://docs.python.org/3/library/itertools.html#itertools.chain" rel="nofollow noreferrer"><code>itertools.chain</code></a>:</p>

<pre><code>from itertools import chain

def process_wrapper(arg_tuple):
    """
    Applies the process function to every line in a chunk of a file, to determine the frequency
    of features in the chunk.
    :param arg_tuple: A tuple that contains: line_process_fun, filename, chunk_start, chunk_size
    :return: A counter object that counts the frequency of each feature in the chunk
    """
    line_process_fun, filename, chunk_start, chunk_size = arg_tuple
    with open(filename) as f:
        f.seek(chunk_start)
        lines = f.read(chunk_size).splitlines()
        return Counter(chain.from_iterable(map(line_process_fun, lines)))
</code></pre></li>
<li><p>Right now you manually have to  unpack <code>line_process_fun, filename, chunk_start, chunk_size = arg_tuple</code>, but if you used <a href="https://docs.python.org/3.4/library/multiprocessing.html?highlight=process#multiprocessing.pool.Pool.starmap" rel="nofollow noreferrer"><code>Pool.starmap</code></a> instead of <code>Pool.map</code>, you could make the signature <code>def process_wrapper(line_process_fun, filename, chunk_start, chunk_size)</code>.</p></li>
<li><p><code>Counter</code> objects support not only updating, but also summing two instances. In this case, quite intuitively, all counts are added. They also have a <a href="https://docs.python.org/3/library/collections.html#collections.Counter.most_common" rel="nofollow noreferrer"><code>most_common</code></a> method which returns tuples of values and counts from most to least counts, so exactly what your reduce and sort does. And finally, <code>sum</code> takes an optional second argument stating what the base object is:</p>

<pre><code>res_list = pool.map(process_wrapper, jobs)
aggregated_count = sum(res_list, Counter()).most_common()
</code></pre>

<p>Make sure to test that this does not slow down the processing, but even if it does, it sure is easier to understand. For the small example given, it is slightly slower on my machine.</p></li>
<li><code>multiprocessing.Pool</code> can also be used as a context manager to ensure it is closed after the processing. This would introduce another level of indentation, though.</li>
</ul>
    </div>