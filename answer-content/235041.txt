<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>I don't have many improvements to offer-- just one major one. Like you suspected, your implementation is not efficient. This is because using a double for loop to set a Torch/NumPy array is not the preferred way to do sum reductions. What is preferred, is the use of <a href="https://pytorch.org/docs/stable/torch.html#torch.einsum" rel="nofollow noreferrer">torch.einsum</a>. It takes an indices equation and reduces the Tensors into a final representation.</p>

<p>First to note is that your equation for <code>reg_g_ij</code> is <strong>not</strong> the most simplified form.</p>

<p>In your code, we start with:</p>

<p><code>q_i * q_j * (1 - C_ij) + (1 - q_i * q_j) * C_ij</code></p>

<p>But it can be reduced to:</p>

<p><code>q_i * q_j * (1 - 2 * C_ij) + C_ij</code></p>

<p>You can prove it yourself with a few lines of algebra.</p>

<p>The last small thing is call <code>.unsqueeze(0)</code> when you're expanding the dimensions of an array. In this case we used this method to expand an array's size from (9, 9) to (1, 9, 9). </p>

<pre class="lang-py prettyprint-override"><code>    A_0 = torch.eye(n_node).unsqueeze(0)
    A_i = A

    B = A_0.repeat(reg_sig.size(0), 1, 1)

    for i in range(1, n_node):
        A_i = Sig(100 * (torch.bmm(A_i, A) - 0.5))

        B += A_i

    C = Sig(100 * (B - 0.5))

    reg_g_ij = torch.einsum('ij,ik,ijk-&gt;ijk', q, q, 1 - 2 * C) + C
</code></pre>

<p>When profiling this approach, we see a pretty big reduction in time:</p>

<pre><code>In [257]: %timeit new(reg_sig, reg_adj)
1000 loops, best of 5: 745 Âµs per loop

In [258]: %timeit orig(reg_sig, reg_adj)
The slowest run took 4.85 times longer than the fastest. This could mean that an intermediate result is being cached.
100 loops, best of 5: 5.44 ms per loop

</code></pre>
    </div>