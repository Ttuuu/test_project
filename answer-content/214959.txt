<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>For <code>N = 1000</code> (as suggested by the comment), the matrix multiplication takes by far the most time out of the whole algorithm. Largely because it is much slower than it needs to be, but even with Eigen (see below) it still takes over 85% of the time. For lower <code>N</code> that ratio is also lower.</p>

<p>The implementation of matrix multiplication is very slow compared to what is possible. It is a transcription of the basic definition of matrix multiplication without any optimization applied. To write a good implementation from scratch it would take, at a high level:</p>

<ol>
<li>A highly optimized inner loop.</li>
<li>Tuned tiling, to avoid excessive cache misses.</li>
<li>Repacking tiles, to avoid excessive TLB misses.</li>
</ol>

<p>Points 2 and 3 are necessary for "large enough" matrixes only, 1000x1000 certainly is large enough, creating a lot of additional complexity..</p>

<p>Splitting out point 1, that would in turn mean:</p>

<ul>
<li>Using SIMD. Neglecting to use SIMD immediately puts the code at a huge disadvantage compared to what is achievable.</li>
<li>Ensuring there are enough <em>independent</em> chains of calculation. Any modern CPU has a good throughput for floating point calculations, in that sense FP math is not slow. However, these operations take a lot of time individually, so the only way to get high throughput is by overlapping the execution of independent operations. For example on Haswell you would need at least 10 independent fused multiply-adds.</li>
<li>Reducing the number of loads. Most modern CPUs can do 2 loads and 2 FMAs per cycle. That means that in order to saturate it with FMAs, the number of loads must not exceed the number of FMAs. In practice it is often good to reduce the ratio of loads to FMAs even further.</li>
<li>Low-level code tuning.</li>
</ul>

<p>You could do these things manually (if you want I can show you some things), but a simple way to improve that without using SIMD intrinsics (or assembly code even) is by using an existing efficient implementation, for example from the Eigen library or IntelÂ® MKL or some other competitor.</p>

<p>A "minimal changes" way to use Eigen here is just converting <code>Y</code> and <code>Jac</code> into <code>Eigen::MatrixXd</code> objects, multiplying them, and extracting the result into <code>dXdt</code>. It is probably better to avoid your <code>dim2</code> type entirely though, to avoid some re-packing and also because nested vectors are already not the most efficient matrix representation.</p>
    </div>