<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h2>Taking Benefits of The Out-of-Order Execution Engine</h2>
<p>You can also read about The Out-of-Order Execution Engine
in the "IntelÂ® 64 and IA-32 Architectures Optimization Reference Manual"
<a href="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" rel="nofollow noreferrer">http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf</a> section the 2.1.2, and take benefits of it.</p>
<p>For example, in Intel SkyLake processor series (launched in 2015), it has:</p>
<ul>
<li>4 execution units for the Arithmetic logic unit (ALU) (add, and, cmp, or, test, xor, movzx, movsx, mov, (v)movdqu, (v)movdqa, (v)movap*, (v)movup),</li>
<li>3 execution units for Vector ALU ( (v)pand, (v)por, (v)pxor, (v)movq, (v)movq, (v)movap*, (v)movup*, (v)andp*, (v)orp*, (v)paddb/w/d/q, (v)blendv*, (v)blendp*, (v)pblendd)</li>
</ul>
<p>So we can occupy above units (3+4) in parallel if we use register-only operations. We cannot use 3+4 instructions in parallel for memory copy. We can use simultaneously maximum of up to two 32-bytes instructions to load from memory and one 32-bytes instructions to store from memory, and even if we are working with Level-1 cache.</p>
<p>Please see the Intel manual again to understand on how to do the fastest memcpy implementation: <a href="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" rel="nofollow noreferrer">http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf</a></p>
<p>Section 2.2.2 (The Out-of-Order Engine on the Haswell micro-architecture): "The Scheduler controls the dispatch of micro-ops onto the dispatch ports. There are eight dispatch ports to support the out-of-order execution core. Four of the eight ports provided execution resources for computational operations. The other 4 ports support memory operations of up to two 256-bit load and one 256-bit store operation in a cycle."</p>
<p>Section 2.2.4 (Cache and Memory Subsystem) has the following note: "First level data cache supports two load micro-ops each cycle; each micro-op can fetch up to 32-bytes of data."</p>
<p>Section 2.2.4.1 (Load and Store Operation Enhancements) has the following information: The L1 data cache can handle two 256-bit (32 bytes) load and one 256-bit (32 bytes) store operations each cycle. The unified L2 can service one cache line (64 bytes) each cycle. Additionally, there are 72 load buffers and 42 store buffers available to support micro-ops execution in-flight.</p>
<p>The other sections (2.3 and so on, dedicated to Sandy Bridge and other microarchitectures) basically reiterate the above information.</p>
<p>The section 2.3.4 (The Execution Core) gives additional details.</p>
<p>The scheduler can dispatch up to six micro-ops every cycle, one on each port. The following table summarizes which operations can be dispatched on which port.</p>
<ul>
<li>Port 0: ALU, Shift, Mul, STTNI, Int-Div, 128b-Mov, Blend, 256b-Mov</li>
<li>Port 1: ALU, Fast LEA, Slow LEA, MUL, Shuf, Blend, 128bMov, Add, CVT</li>
<li>Port 2 &amp; Port 3: Load_Addr, Store_addr</li>
<li>Port 4: Store_data</li>
<li>Port 5: ALU, Shift, Branch, Fast LEA, Shuf, Blend, 128b-Mov, 256b-Mov</li>
</ul>
<p>The section 2.3.5.1 (Load and Store Operation Overview) may also be useful to understand on how to make fast memory copy, as well as the section 2.4.4.1 (Loads and Stores).</p>
<p>For the other processor architectures, it is again - two load units and one store unit. Table 2-4 (Cache Parameters of the Skylake Microarchitecture) has the following information:</p>
<p>Peak Bandwidth (bytes/cyc):</p>
<ul>
<li>First Level Data Cache: 96 bytes (2x32B Load + 1*32B Store)</li>
<li>Second Level Cache: 64 bytes</li>
<li>Third Level Cache: 32 bytes.</li>
</ul>
<p>I have also done speed tests on my Intel Core i5 6600 CPU (Skylake, 14nm, released in September 2015) with DDR4 memory, and this has confirmed the theory. For example, my tests have shown that using generic 64-bit registers for memory copy, even many registers in parallel, degrades performance, comparing to larger registers (XMM). Also, using just 2 XMM registers is enough - adding the 3rd doesn't add performance.</p>
<p>If your CPU has AVX CPUID bit, you may take benefits of the large, 256-bit (32 byte) YMM registers to copy memory, to occupy two full load units. The AVX support was first introduced by Intel with the Sandy Bridge processors, shipping in Q1 2011 and later on by AMD with the Bulldozer processor shipping in Q3 2011.</p>
<pre><code>// first cycle - use two load  units
vmovdqa  ymm0, ymmword ptr [esi+0]       // load first part (32 bytes)
vmovdqa  ymm1, ymmword ptr [esi+32]      // load 2nd part (32 bytes)

// second cycle - use one load unit and one store unit
vmovdqa  xmm2, xmmword ptr [esi+64]      // load 3rd part (16 bytes)
vmovdqa  ymmword ptr [edi+0],  ymm0      // store first part

// third cycle - use one store unit
vmovdqa  ymmword ptr [edi+32], ymm1      // store 2nd part

// fourth cycle - use one store unit
vmovdqa  xmmword ptr [edi+64], xmm2      // store 3rd part
</code></pre>
<p>Just make sure your data is aligned by 16 bytes (for the XMM registers) and by 32 bytes (for the YMM registers), otherwise there will be an Access Violation error. If the data is not aligned, use unaligned commands: vmovdqu and movups respectively.</p>
<p>If you are lucky to have an AVX-512 processor, you can copy 80 bytes in just four instructions:</p>
<pre><code>vmovdqu64   zmm30, [esi]
vmovdqu     xmm31, [esi+64]       
vmovdqu64   [edi], zmm30
vmovdqu     [edi+64], xmm31     
</code></pre>
<p>We are using registers 30 and 31 here to not enter the <em>upper 256 dirty state</em> which is a global state, which may incur SSE/AVX transitional penalties, moreover, on some CPU models <code>vzeroupper</code> or <code>vzeroall</code> are the only way to exit this state or even restore <em>max-turbo</em> after dirtying a ZMM register. The CPU, however, will not enter this state for writes to (x/y/z)mm16-31 - registers which do not exist on SSE/AVX1/AVX2.</p>
<h2>Further reading - ERMSB (not needed to copy exactly 80 bytes but for much larger blocks)</h2>
<p>If your CPU has CPUID ERMSB (Enhanced REP MOVSB) bit, then <code>rep movsb</code> command is executed differently than on older processors, and it will be faster than <code>rep movsd</code> (<code>movsq</code>), however the benefits of <code>rep movsb</code> will only be only noticeable on large blocks.</p>
<p><code>rep movsb</code> is faster than plain simple "<code>mov rax</code> in a loop" copy only starting form 256-byte blocks, and faster then AVX copy starting from 2048 bytes-blocks.</p>
<p>So, since your block size is 80 bytes only, ERMSB will not give you any benefit.</p>
<p>Get Microsoft Visual Studio, and look for memcpy.asm - it has different scenarios for different processors and different block sizes - so you will be able to figure out which method is best to use for your processor and your block size.</p>
<p>In the meanwhile, I can consider Intel ERMSB "half-baked", because there is high internal startup in ERMSB - about 35 cycles, and because of the other limitations.</p>
<p>See the Intel Manual on Optimization, section 3.7.6 Enhanced REP MOVSB and STOSB operation (ERMSB) <a href="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" rel="nofollow noreferrer">http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf</a></p>
<ul>
<li>startup cost is 35 cycles;</li>
<li>both the source and destination addresses have to be aligned to a 16-Byte boundary;</li>
<li>the source region should not overlap with the destination region;</li>
<li>the length have to be a multiple of 64 to produce higher performance;</li>
<li>the direction have to be forward (<code>CLD</code>).</li>
</ul>
<p>I hope that in future Intel will eliminate such a high startup costs.</p>
    </div>