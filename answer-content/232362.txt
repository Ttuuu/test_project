<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Loops over large arrays are not really a good idea in Python. This is why your original list comprehension is not terribly fast.</p>

<p>Your numpy version is loop free, but as far as I know, <code>np.repeat</code> actually makes copies of your data, which again, is really inefficient. An alternative would be to use <code>np.tile</code>, which maybe does not need to copy the data. But we don't really need to bother since numpy has a great feature called <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html" rel="noreferrer"><em>broadcasting</em></a>, which often makes <code>np.repeat</code>/<code>np.tile</code> completely unneccessary. Broadcasting basically does <code>np.repeat/tile</code> automatically.</p>

<p>To evaluate the performance, I created a more abstract version of your list comprehension:</p>

<pre><code>def get_valid_op(arr, lowers, uppers):
    return np.asarray([any((val &gt;= lowers) &amp; (val &lt; uppers)) for val in arr])
</code></pre>

<p>and also a broadcasting version</p>

<pre><code>def get_valid_arr(arr, lowers, uppers):
    valid = np.logical_and(arr.reshape(1, -1) &gt;= lowers.reshape(-1, 1), arr.reshape(1, -1) &lt; uppers.reshape(-1, 1))
    return valid.any(axis=0)
</code></pre>

<p>The second one is virtually the exact same algorithm as your repeat/reshape code.</p>

<p>With some test data modeled after your description above</p>

<pre><code>arr = np.linspace(0, 1000, 70000)
starts = np.linspace(0, 150, 151) * 400
ends = starts + np.random.randint(0, 200, region_starts.shape)  # I assumed non-overlapping regions here
</code></pre>

<p>we can first <code>assert all(get_valid_op(arr, starts, ends) == get_valid_arr(arr, starts, ends))</code> and then time:</p>

<pre class="lang-none prettyprint-override"><code>%timeit -n 10 get_valid_op(arr, starts, ends)
511 ms ± 5.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit -n 10 get_valid_arr(arr, starts, ends)
37.8 ms ± 3.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>

<p>An order of magnitude faster. Not bad to begin with ;-)</p>

<p>Since working with large arrays (<code>valid</code> has a shape of <code>(150, 70000)</code> before reduction) also has a cost, I then took a step back and returned to loopy-land (just a little bit). </p>

<pre><code>def get_valid_loop(arr, lowers, uppers):
    valid = np.zeros(arr.shape, dtype=bool)
    for start, end in zip(lowers, uppers):
        valid = np.logical_or(valid, np.logical_and(start &lt;= arr, arr &lt; end))
    return valid
</code></pre>

<p>In contrast to your list comprehension, this version now only iterates over the shorter region limit vectors, which means about two orders of magnitude fewer iterations.</p>

<p>We can then again <code>assert all(get_valid_op(arr, starts, ends) == get_valid_loop(arr, starts, ends))</code> and time it:</p>

<pre class="lang-none prettyprint-override"><code>%timeit -n 10 get_valid_loop(arr, starts, ends)
18.1 ms ± 865 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>

<p>As the results show, this version is even faster on my "synthetic" benchmark inputs.</p>

<p>In the end you will have to check the versions in your application and see which one performs best.</p>
    </div>