<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h1>Header size</h1>
<p>256*4 bytes is very big for a header. The size could be reduced substantially by using one or several of these common techniques:</p>
<ul>
<li>Store the <em>code length</em> instead of symbol frequency. These definitely won't need 32 bits each, 8 would already be a lot. You can pack them in 4 bits each if you set a length limit of 15. Storing lengths is not ambiguous because you can use <a href="https://en.wikipedia.org/wiki/Canonical_Huffman_code" rel="noreferrer">canonical Huffman codes</a> (there is an easy algorithm to generate them from your table of code lengths, discarding the code itself).</li>
<li>Compress the header with delta encoding: storing the length difference between subsequent codes, using a variable-length encoding. Small differences tend to be more common. (seen in eg DEFLATE)</li>
<li>Remove most zero-lengths from the header, by first storing a sparse bitmap that indicates which symbols occur in the file. (seen in eg bzip2)</li>
</ul>
<h1>Encoding process</h1>
<p>Walking up the tree for every byte of the file is needlessly inefficient. You could precompute an array of codes and lengths once in advance and then use the array during encoding. The code could be represented as a single unsigned integer, no array necessary (it won't be that long, and you will want to limit the code lengths anyway for decoding and header reasons). It can be prepended to <code>buf</code> in a couple of simple bitwise operations, similar to how you currently add a single bit, but <code>nbuf++</code> turns into <code>nbuf += codelength</code>. Together this lets the encoding process take a constant number of operations instead of scaling linearly with the code length.</p>
<h1>Decoding process</h1>
<p>Currently your code implements bit-by-bit tree walking, which is (as one <a href="http://www.compressconsult.com/huffman/#decoding" rel="noreferrer">source</a> puts it) dead slow. The alternative is decoding several bits at the same time by using an array lookup. There are a lot of subtly different ways to do that, but the basis of all of them is that part of the buffer is used to index into a table. Limiting the maximum length of the codes is very useful, because with a limited length you can guarantee that decoding is a <em>single-step process</em>, resolving one symbol from the buffer in a constant number of operations, with no looping.</p>
<p>Some possible relevant sources for these techniques are the one in the previous paragraph and:</p>
<ul>
<li><a href="https://github.com/IJzerbaard/shortarticles/blob/master/huffmantable.md" rel="noreferrer">Introduction to table based Huffman decoding</a></li>
<li><a href="https://www.researchgate.net/publication/262283882_An_efficient_algorithm_of_Huffman_decoder_with_nearly_constant_decoding_time" rel="noreferrer">An efficient algorithm of Huffman decoder with nearly constant decoding time</a></li>
<li><a href="http://fastcompression.blogspot.com/2015/07/huffman-revisited-part-2-decoder.html" rel="noreferrer">Huffman revisited - Part 2 : the Decoder</a></li>
<li><a href="https://www.researchgate.net/publication/221544036_A_Fast_and_Space_-_Economical_Algorithm_for_Length_-_Limited_Coding" rel="noreferrer">A Fast and Space - Economical Algorithm for Length - Limited Coding</a> (for a way to generate the code lengths with a length limit)</li>
</ul>
    </div>