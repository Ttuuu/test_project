<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h1>Using the Wikipedia API</h1>

<p>As mentioned in another answer, Wikipedia provides an HTTP API for fetching article content, and you can use it to get the content in much cleaner formats than HTML. Often this is much better (for example, it's a much better choice for <a href="https://github.com/geajack/what-is" rel="nofollow noreferrer">this little project I wrote</a> that extracts the first sentence of a Wikipedia article).</p>

<p>However, in your case, you have to parse tables anyway. Whether you're parsing them from HTML or from the Wikipedia API's "wikitext" format, I don't think there's much of a difference. So I consider this subjective.</p>

<h1>Using requests, not urllib</h1>

<p>Never use urllib, unless you're in an environment where for some reason you can't install external libraries. The <code>requests</code> library is preferred for fetching HTML.</p>

<p>In your case, to get the HTML, the code would be:</p>

<pre><code>html = requests.get(url).text
</code></pre>

<p>with an <code>import requests</code> up-top. For your simple example, this isn't actually any easier, but it's just a general best-practice with Python programming to always use <code>requests</code>, not <code>urllib</code>. It's the preferred library.</p>

<h1>What's with all those with blocks?</h1>

<p>You don't need three <code>with</code> blocks. When I glance at this code, three <code>with</code> blocks make it seem like the code is doing something much more complicated than it actually is - it makes it seem like maybe you're writing multiple CSVs, which you aren't. Just use one, it works the same:</p>

<pre><code>with open('data.csv', 'w',encoding='UTF-8', newline='') as f:
    fields = ['Title', 'Year']
    writer = csv.writer(f, delimiter=',')
    writer.writerow(fields)

    for tr in My_table.find_all('tr')[2:]: # [2:] is to skip empty and header
        tds = tr.find_all('td')
        try:
            title = tds[0].text.replace('\n','')
        except:
            title = ""
        try:
            year = tds[2].text.replace('\n','')
        except:
            year = ""

        writer.writerow([title, year])

    for tr in My_second_table.find_all('tr')[2:]: # [2:] is to skip empty and header
        tds = tr.find_all('td')
        row = "{}, {}".format(tds[0].text.replace('\n',''), tds[2].text.replace('\n',''))
        writer.writerow(row.split(','))
</code></pre>

<h1>Are those two tables really that different?</h1>

<p>You have two for loops, each one processing a table and writing it to the CSV. The body of the two for loops is different, so at a glance it looks like maybe the two tables have a different format or something... but they don't. You can copy paste the body of the first for loop into the second and it works the same:</p>

<pre><code>for tr in My_table.find_all('tr')[2:]: # [2:] is to skip empty and header
    tds = tr.find_all('td')
    try:
        title = tds[0].text.replace('\n','')
    except:
        title = ""
    try:
        year = tds[2].text.replace('\n','')
    except:
        year = ""

    writer.writerow([title, year])

for tr in My_second_table.find_all('tr')[2:]: # [2:] is to skip empty and header
    tds = tr.find_all('td')
    try:
        title = tds[0].text.replace('\n','')
    except:
        title = ""
    try:
        year = tds[2].text.replace('\n','')
    except:
        year = ""
</code></pre>

<p>With the above code, the only difference to the resulting CSV is that there aren't spaces after the commas, which I assume is not really important to you. Now that we've established the code doesn't need to be different in the two loops, we can just do this:</p>

<pre><code>table_rows = My_table.find_all('tr')[2:] + My_second_table.find_all('tr')[2:]

for tr in table_rows:
    tds = tr.find_all('td')
    try:
        title = tds[0].text.replace('\n','')
    except:
        title = ""
    try:
        year = tds[2].text.replace('\n','')
    except:
        year = ""

    writer.writerow([title, year])
</code></pre>

<p>Only one for loop. Much easier to understand!</p>

<h2>Parsing strings by hand</h2>

<p>So that second loop doesn't need to be there at all, but let's look at the code inside of it anyway:</p>

<pre><code>row = "{}, {}".format(tds[0].text.replace('\n',''), tds[2].text.replace('\n',''))
writer.writerow(row.split(','))
</code></pre>

<p>Um... you just concatenated two strings together with a comma, just to call <code>split</code> and split them apart at the comma at the very next line. I'm sure now it's pointed out to you you can see this is pointless, but I want to pull you up on one other thing in these two lines of code.</p>

<p>You are essentially trying to parse data by hand with that <code>row.split</code>, which is <strong>always dangerous</strong>. This is an important and general lesson about programming. What if the name of the chip had a comma in it? Then <code>row</code> would contain more commas than just the one that you put in there, and your call to <code>writerow</code> would end up inserting more than two columns!</p>

<p><strong>Never</strong> parse data by hand unless you absolutely have to, and <strong>never</strong> write data in formats like CSV or JSON by hand unless you absolutely have to. <strong>Always</strong> use a library, because there are always pathological edge cases like a comma in the chip name that you won't think of and which will break your code. The libraries, if they're been around for a while, have had those bugs ironed out. With these two lines:</p>

<pre><code>row = "{}, {}".format(tds[0].text.replace('\n',''), tds[2].text.replace('\n',''))
writer.writerow(row.split(','))
</code></pre>

<p>you are attempting to split a table row into its two columns <em>yourself</em>, by hand, which is why you made a mistake (just like anyone would). Whereas in the first loop, the code which does this splitting are the two lines:</p>

<pre><code>title = tds[0].text.replace('\n','')
year = tds[2].text.replace('\n','')
</code></pre>

<p>Here you are relying on BeautifulSoup to have split the columns cleanly into <code>tds[0]</code> and <code>tds[2]</code>, which is much safer and is why this code is much better.</p>

<h1>Mixing of input parsing and output generating</h1>

<p>The code that parses the HTML is mixed together with the code that generates the CSV. This is poor breaking of a problem down into sub-problems. The code that writes the CSV should just be thinking in terms of titles and years, it shouldn't have to know that they come from HTML, and the code that parses the HTML should just be solving the problem of extracting the titles and years, it should have no idea that that data is going to be written to a CSV. In other words, I want that for loop that writes the CSV to look like this:</p>

<pre><code>for (title, year) in rows:
    writer.writerow([title, year])
</code></pre>

<p>We can do this by rewriting the with block like this:</p>

<pre><code>with open('data.csv', 'w',encoding='UTF-8', newline='') as f:
    fields = ['Title', 'Year']
    writer = csv.writer(f, delimiter=',')
    writer.writerow(fields)

    table_rows = My_table.find_all('tr')[2:] + My_second_table.find_all('tr')[2:]
    parsed_rows = []
    for tr in table_rows:
        tds = tr.find_all('td')
        try:
            title = tds[0].text.replace('\n','')
        except:
            title = ""
        try:
            year = tds[2].text.replace('\n','')
        except:
            year = ""
        parsed_rows.append((title, year))

    for (title, year) in parsed_rows:
        writer.writerow([title, year])
</code></pre>

<h1>Factoring into functions</h1>

<p>To make the code more readable and really separate the HTML stuff from the CSV stuff, we can break the script into functions. Here's my complete script.</p>

<pre><code>import requests
from bs4 import BeautifulSoup
import csv

urls = ['https://en.wikipedia.org/wiki/Transistor_count']
data = []

def get_rows(html):
    soup = BeautifulSoup(html,'html.parser')
    My_table = soup.find('table',{'class':'wikitable sortable'})
    My_second_table = My_table.find_next_sibling('table')
    table_rows = My_table.find_all('tr')[2:] + My_second_table.find_all('tr')[2:]
    parsed_rows = []
    for tr in table_rows:
        tds = tr.find_all('td')
        try:
            title = tds[0].text.replace('\n','')
        except:
            title = ""
        try:
            year = tds[2].text.replace('\n','')
        except:
            year = ""
        parsed_rows.append((title, year))

    return parsed_rows

for url in urls:
    html = requests.get(url).text
    parsed_rows = get_rows(html)

    with open('data.csv', 'w',encoding='UTF-8', newline='') as f:
        fields = ['Title', 'Year']
        writer = csv.writer(f, delimiter=',')
        writer.writerow(fields)
        for (title, year) in parsed_rows:
            writer.writerow([title, year])
</code></pre>

<p>What would be even better would be for <code>get_rows</code> to be a generator rather than a regular function, but that's advanced Python programming. This is fine for now.</p>
    </div>