<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>If I understand your problem, your samples are 150-dimensional vectors
which is to be classified into one of four categories. If so, an LSTM
architecture is a very poor fit because there are no relations
between your samples. I.e what is in the n:th sample doesn't impact
what is in the n + 1:th sample.</p>

<p>Try something like this instead:</p>

<pre><code>from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical
import numpy as np

n = 38607
d = 150
k = 4

# Generate data
X = np.random.randn(n, d)
Y = to_categorical(np.random.randint(k, size = n), k)

model = Sequential()
model.add(Dense(128, input_dim = d, activation = 'relu'))
model.add(Dense(64, activation = 'relu'))
model.add(Dense(4, activation = 'softmax'))
model.compile(
    loss = 'binary_crossentropy',
    optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(X, Y, epochs = 100, batch_size = 128, verbose = 1)
</code></pre>

<p>I haven't used any test data so the model quickly overfits (accuracy &gt;
25% implies overfitting). Which brings me to my next point. On your
graphs, you get better performance on the test data than on the
training data which is very suspect. I suggest you try and train your
network without any dropout first to see if it behaves as expected
before adding it back.</p>
    </div>