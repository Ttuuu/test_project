<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Before we dive into code, I'd like to ask: Does this CSV really need to be generated on-the-fly/on-demand? If the answer is <em>no</em>, you can probably run a cron job using <a href="https://docs.mongodb.com/manual/reference/program/mongoexport/" rel="nofollow noreferrer">mongoexport</a>. This way, you avoid Node altogether.</p>

<p>Libraries aren't immune to memory limits. They create objects too! In this case, it starts when you loaded up a lot of <code>Follow</code> entries into <code>follows</code>. This is compounded when you converted all of that data into CSV. Under the hood, you 1) loaded a lot of data into an huge array of objects and 2) you converted that huge array of objects into a huge string.</p>

<p>Now luckily, <a href="https://github.com/zeMirco/json2csv#json2csv-transform-streaming-api" rel="nofollow noreferrer"><code>json2csv</code> has a streaming API</a>. What this means is that, instead of processing all of your results in one go in memory, you have the option to build that CSV by chunk. Since we're dealing with an array of objects instead of strings, buffers and arrays of raw data, you should look at the "object mode".</p>

<p>So what you do is set up a pipeline - a bunch of functions connected together and called one after the other with the output of the previous being the input of the next. Data is streamed into this pipeline, transforming on every transform function it passes through until it reaches the end.</p>

<p>In your case, it would look <em>something like</em> this:</p>

<pre><code>const { createWriteStream } = require('fs')
const { Readable } = require('stream')
const { Transform } = require('json2csv')

// ...somewhere in your controller code...

// With streams, everything starts with and ends with a stream. This is the start.
const input = new Readable({ objectMode: true })
input._read = () =&gt; {} // Not sure what this is for, really.

// We set up the transformer, the thing that converts your object into CSV rows.
const json2csv = new Transform ({ fields }, { objectMode: true })

// Create a stream to the file. This is the end of the line.
const output = createWriteStream('./output')

// You can optionally listen to the output when all the data is flushed
output.on('finish', () =&gt; { /* all done */ })

// We connect the dots. So this reads like:
// 1. Read data into input.
// 2. Input gets fed into json2csv.
// 3. The output of json2csv is fed into output (which is our file).
const stream = input.pipe(json2csv).pipe(output)

// Start pumping data into the stream. You could use setInterval if you wanted.
follows.forEach(o =&gt; input.push(obj))

// Close the input.
input.push(null) 
</code></pre>

<p>Now I'm not too familiar with Mongoose. But if it has an API that exposes your results as a stream, you can skip object mode and pipe that stream to <code>json2csv</code> instead. This way, the entire thing uses streams and at no point would all data be stored in memory. They get loaded, processed and flushed a few pieces at a time.</p>

<pre><code>const stream = streamFromMongoose.pipe(json2csv).pipe(output)
</code></pre>
    </div>