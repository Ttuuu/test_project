<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<h3>Quick skim results:</h3>

<ul>
<li>The indentation is off. This might be an artefact of how you pasted the code into the question, though.</li>
<li>Inconsistent use of newlines at the start and end of blocks.<br>
Sometimes you start a block and put an empty line at its start, sometimes you just don't put a newline there. The same goes for the end of blocks.<br>
You should be able to configure your formatting preferences to take care of that for you :)</li>
<li>There's some leftover auto-generated TODO comments. If they are fixed, drop them :)</li>
</ul>

<h3>Bugs!</h3>

<ul>
<li><p>When any item in your logfile does not match its filter criteria, all the filter methods return an empty list. I don't think that's desired:</p>

<p>Consider the following logfile:</p>

<pre><code>TIMESTAMP,COUNTRY_CODE,RESPONSE_TIME
1425859632,US,500
1452145245,GB,137
</code></pre>

<p>If you now call <code>filterByCountry(logfile, "US")</code>, the result is an empty list. That's most definitively not desired!</p></li>
<li><p>There is a slim possibility that the caller of the method expects the Reader it passes to remain open. As of now you're closing the BufferedReader. Per the spec, closing a BufferedReader must also close its underlying Reader. This should not be much of an issue here, because you probably expected the method to consume the whole file anyways, but it's something to be aware of.<br>
It becomes significantly more relevant when you're handling Writers that follow the same mechanic, but are usually intended to be kept open.</p></li>
</ul>

<h3>General approach:</h3>

<p>Your overall approach is most likely very slow. You're parsing possibly extreme quantities of files each time you want to filter by anything. If you run these "queries" a lot, you should strongly consider aggregating the log entries in a database.<br>
Databases are explicitly designed for querying and aggregation of high volumes of data. This could give you an extreme boost to performance. All RDBMS I know of support importing from CSV.</p>

<p>You're also not making use of Object orientation at all. Currently log entries are stored contiguously in a List. This means that any code that consumes the log entries must know how many fields an entry has when iterating over that list. It also means that any change to the structure of your logging entries requires changes in the code using the results from the presented classes, that may not be obvious or even easy to find.</p>

<p>To avoid this, you should model a LogEntry in a class, somewhat like this:</p>

<pre><code>@Value
public class LogEntry {
    private long timestamp;
    private String countryCode; // possibly Locale?
    private int responseTime;
}
</code></pre>

<blockquote>
  <p>Sidenote: I'm using Lombok's @Value annotation here to automatically generate getters and a constructor for me. </p>
</blockquote>

<p>This allows you to make your filter methods return a significantly cleaner (and nicer) datatype than <code>Collection&lt;?&gt;</code> with contiguously stored data. Instead we get <code>Collection&lt;LogEntry&gt;</code>.</p>

<p>This also exposes an issue with how you parse the log entries for filtering on RESPONSE_TIME. Currently you just assume that the response time is always less than <code>1000000000</code>. That is probably a reasonable assumption. You also assume that it's the only value in the log entry that only contains digits and is shorter than 10 characters. That may be  a less reasonable assumption (especially if the log-entry format changes).<br>
To find it in your entry you iterate all values in each entry and manually check each character for being numeric and then check the length of the entry as well. If the columns in your logfiles are always the same, that makes no sense. You know the columns, you know the index they have, make use of that knowledge.<br>
If you can only guarantee that the log entry columns are consistent in a single file and the header indicates how they are arranged, you should be using the header to determine the indices of your columns.</p>

<p>Consider something like the following:</p>

<pre><code>public static Collection&lt;LogEntry&gt; filterByCountry(Reader source, String country) throws IOException {
    BufferedReader br = new BufferedReader(source);
    int timestampCol = -1;
    int countryCol = -1;
    int responseTimeCol = -1;
    String line = br.readLine();
    // File completely empty, not even a header
    if (line == null) {
        return Collections.emptyList();
    }
    String[] headers = line.split(",");
    for (int i = 0; i &lt; headers.length; i++) {
        String h = headers[i];
        if (h.equals("TIMESTAMP")) {
            timestampCol = i; continue;
        }
        if (h.equals("COUNTRY_CODE")) {
            countryCol = i; continue;
        }
        if (h.equals("RESPONSE_TIME")) {
            responseTimeCol = i; continue;
        }
    }
    Collection&lt;LogEntry&gt; result = new List&lt;&gt;();
    while ((line = br.readLine()) != null) {
        String[] logEntry = line.split(",");
        LogEntry entry = new LogEntry(Long.parseLong(logEntry[timestampCol])
            , logEntry[countryCol]
            , Integer.parseInt(logEntry[responsetimeCol]));
        if (entry.getCountry().equals(country)) {
            result.add(entry);
        }
    }
    return result;
}
</code></pre>
    </div>