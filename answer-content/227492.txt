<div class="s-prose s-prose__disable-spoiler-hover js-post-body" itemprop="text">
<p>Maybe the code is time efficient because you are lucky. Maybe the code is time efficient because you have good intuitions. This review is about moving toward luck playing less of a role in the performance of your code and/or reaching good intuitive conclusions by a more formal path.</p>
<h2>Top Down</h2>
<p>The code seems to be written bottom up from loops rather than top down from the specification. That makes it hard to reason about how it solves the problem. <a href="https://channel9.msdn.com/Events/Build/2014/3-642" rel="nofollow noreferrer">Leslie Lamport recommends starting with a specification</a>.</p>
<pre><code>f (arrayOfCharacters: I, alphabet: A) -&gt; positiveInteger: Result
Result = Max(S)
S = Sequence of positiveIntegers (S0,S1...Sk):
    where Sk is the length of the k'th substring of I
</code></pre>
<p>This is enough to reason about the <em>best</em> case performance. Let <code>N = I.size</code>. If all the characters in I are the same then there are only N-1 <strong>character lookups</strong> and:</p>
<pre><code>Result = 1
S.size = I.size
</code></pre>
<p>The running time depends on the character of <strong>character lookups</strong>.</p>
<h2>The intution for O(n<sup>2</sup>)</h2>
<p>It is possible to have an input <code>I</code> that takes <code>I.length^2</code> time to run.</p>
<pre><code>A is the alphabet
I is the input stream of characters
N is the length of I
A has more than N characters.
Each character in I is unique.
</code></pre>
<p>In that case the longest substring without repeating characters is I and each character of I has to be compared against every other character.</p>
<h2>On the other hand</h2>
<p>The alphabet <code>A</code> is bounded. That's what makes it an alphabet and the input <code>I</code> a string. For character <em>C<sub>i</sub></em> in input I, the maximum number of comparisons is the size of the alphabet <code>A</code>. The size of the alphabet is a constant and the worst case runtime is:</p>
<pre><code>Min(N*N, N*A.size)
</code></pre>
<p>The worst case is <code>N*A.size</code> when <code>N &lt; A.size</code>. In Big-O notation though <code>N*A.size</code> is O(n) implementing an O(n<sup>2</sup>) may be better when the <code>A.size</code> is large. For example when <code>A</code> is the set of <em>all possible</em> unicode codepoints (1,114,112) and <code>N</code> is 1000.</p>
<h2>Going further</h2>
<p>The actual number of <em>assigned</em> unicode codepoints (&lt;300,000) is less than the set of <em>all possible</em> unicode codepoints (1,114,112). Using assigned rather than possible code points might improve speed about 3x.</p>
<p>Similarly, the actual number of unique unicode codepoints when <code>I.length = 1000</code> is less than 1001. So the worst case for <code>I</code> becomes:</p>
<pre><code>Min(N*N, N*A.size, N*I.unique.size)
</code></pre>
<p><code>I.unique.size</code> is not larger than <code>A.size</code>. But because <code>I.unique.size</code> is of order <code>N</code> the Big-O is O(n<sup>2</sup>). Again, O(n<sup>2</sup>) may be better than guaranteed O(n). It will be faster when the cost of determining uniques is less than the cost of scanning the entire alphabet for each substring. Or from a practical standpoint when benchmarks against actual data show it is faster. That's engineering versus computer science. Without actual inputs <code>I</code> and without knowing the alphabet for <code>A</code> we have to go with computer science.</p>
<h2>Recursion</h2>
<p>The question mentions proofs. Often, proofs are inductive. Because there is a mapping between induction and recursing a list, a recursive procedure may be easier to reason about for a person comfortable with proofs. In particular recursion can be applied to the generation of the list of substring lengths <code>S</code>:</p>
<pre><code># List(character), List(Int), Substring -&gt; List(Int)
define Loop(I, S, substring)
  # no more input
  if I = []
    return S
  # end of substring
  else if I.first in substring 
    Loop(I.rest,
         S.append(substring.size),
         New substring)
  else
    Loop(I.rest,
    S,
    substring.append(I.first)
end define
</code></pre>
<p>This makes the top level call:</p>
<pre><code># List(character) -&gt; Integer
define maxSubstringLength(I)
  return max(Loop(I.rest, [], New Substring.append(I.first)))
end define
</code></pre>
<p>The top level call looks a lot like the top level of the specification and the Loop looks a lot like the interior of the specification. It's somewhat obvious that both run in O(n) time in and of themselves.</p>
<h2>Localizing bottlenecks</h2>
<p>All of the variation in performance is the implementation of <code>in substring</code> within <code>Loop</code> at:</p>
<pre><code>  # end of substring
  else if I.first in substring
</code></pre>
<p>And we are free to implement it whatever way works best for whatever particular combination of data and alphabet we are actually dealing with. For example <code>New Substring</code> might return <code>""</code> or it might return a bitmap of <code>A.size</code> bits or it might return a balanced tree of 32 width or it might return a <code>key:value</code> store etc. Again, it's software engineering versus computer science.</p>
<h2>Review Points</h2>
<ul>
<li>Reasoning about the problem first may facilitate writing code that is easier to reason about.</li>
<li>As pointed out the accepted answer, attention to features of the specific problem domain (alphabets) is likely to provide better insights into performance than a generic solution.</li>
<li>Tweeking performance requires engineering research. The memory requirement for <code>in substring</code> can be reduced at least to <code>A.size</code> bits when the membership test is based on a bitmap. In ASCII that's fifteen bytes. For Unicode it's about 125 kilobytes.</li>
<li>Is encoding the entire space of Unicode into a bitmap good or bad engineering? 125k sounds like a lot of memory compared to pushing 2-4 byte characters into an array. But 125k will often fit into the cache of the CPU. It is harder to reason about data locality when building arrays of seen characters.</li>
<li>It might be hard to really engineer a robust solution to a Leetcode problem if the solution needs to be written in twenty minutes.</li>
</ul>
<p><strong>Aside:</strong> <em>{In so far as recursion makes reasoning about the algorithm easier for you, Python may be a suboptimal choice. Iteration is idiomatic Python and recursion not idiomatic Python. Recursion is not idiomatic in Python because Python does not have tail recursion and recursive method calls may crash on large inputs when iterative methods would progress.}</em></p>
    </div>